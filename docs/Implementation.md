# FinGear ç³»çµ±å¯¦ä½œæ¶æ§‹æ–‡ä»¶

> **æ–‡ä»¶é¡å‹**ï¼šæ¶æ§‹è¨­è¨ˆèˆ‡å¯¦ä½œæŒ‡å°
> **ç‰ˆæœ¬**ï¼šv1.0
> **æœ€å¾Œæ›´æ–°**ï¼š2026-01-02
> **é…å¥—æ–‡ä»¶**ï¼š[Overview.md](Overview.md), [FunctionalIndicators.md](FunctionalIndicators.md)

## å¿«é€Ÿå°è¦½

### æ ¸å¿ƒæ¨¡çµ„ç´¢å¼•

| æ¨¡çµ„                       | ä½ç½®      | èªªæ˜                             |
| -------------------------- | --------- | -------------------------------- |
| [ç³»çµ±æ¶æ§‹ç¸½è¦½](#2-ç³»çµ±æ¶æ§‹ç¸½è¦½) | ç¬¬ 2 ç«    | æ•´é«”æ¶æ§‹ã€æ•¸æ“šæµã€æŠ€è¡“æ£§         |
| [API å®¢æˆ¶ç«¯](#31-api-å®¢æˆ¶ç«¯æ¨¡çµ„) | ç¬¬ 3.1 ç¯€ | Shioaji API å°è£èˆ‡éŒ¯èª¤è™•ç†       |
| [å› å­è¨ˆç®—å¼•æ“](#32-å› å­è¨ˆç®—å¼•æ“) | ç¬¬ 3.2 ç¯€ | 7 å› å­è¨ˆç®—é‚è¼¯èˆ‡ç·©å­˜ç­–ç•¥         |
| [é¸è‚¡ç¯©é¸å™¨](#33-é¸è‚¡ç¯©é¸å™¨)    | ç¬¬ 3.3 ç¯€ | ä¸‰å±¤ç¯©é¸é‚è¼¯èˆ‡ç¶œåˆè©•åˆ†æ¨¡å‹       |
| [æ•¸æ“šç®¡é“](#34-æ•¸æ“šç®¡é“)        | ç¬¬ 3.4 ç¯€ | ETL æµç¨‹èˆ‡ Parquet åˆ†å€ç®¡ç†      |
| [è‡ªå‹•åŒ–è…³æœ¬](#4-è‡ªå‹•åŒ–è…³æœ¬è¨­è¨ˆ) | ç¬¬ 4 ç«    | æ¯æ—¥æ›´æ–°ã€ç­–ç•¥æƒæã€é€šçŸ¥æ¨é€     |
| [æ¸¬è©¦èˆ‡é©—è­‰](#5-æ¸¬è©¦èˆ‡é©—è­‰)     | ç¬¬ 5 ç«    | å–®å…ƒæ¸¬è©¦ã€æ•¸æ“šé©—è­‰ã€å›æ¸¬æ¡†æ¶     |
| [éƒ¨ç½²èˆ‡é‹ç¶­](#6-éƒ¨ç½²èˆ‡é‹ç¶­)     | ç¬¬ 6 ç«    | Docker å®¹å™¨åŒ–ã€ç›£æ§ã€éŒ¯èª¤æ¢å¾©    |

### é—œéµåœ–è¡¨æ¸…å–®

| åœ–è¡¨åç¨±                         | é¡å‹       | ä½ç½®        |
| -------------------------------- | ---------- | ----------- |
| ç³»çµ±æ•´é«”æ¶æ§‹åœ–                   | Mermaid    | ç¬¬ 2.1 ç¯€   |
| æ¨¡çµ„ä¾è³´é—œä¿‚åœ–                   | Mermaid    | ç¬¬ 2.2 ç¯€   |
| æ•¸æ“šæµå‘åœ–                       | Mermaid    | ç¬¬ 2.3 ç¯€   |
| API é€£ç·šæ™‚åºåœ–                   | Sequence   | ç¬¬ 3.1.2 ç¯€ |
| å› å­è¨ˆç®—æµç¨‹åœ–                   | Flowchart  | ç¬¬ 3.2.1 ç¯€ |
| ä¸‰å±¤ç¯©é¸ç‹€æ…‹æ©Ÿåœ–                 | State      | ç¬¬ 3.3.2 ç¯€ |
| ETL ç®¡é“æµç¨‹åœ–                   | Flowchart  | ç¬¬ 3.4.1 ç¯€ |
| æ¯æ—¥æ›´æ–°åŸ·è¡Œæµç¨‹åœ–               | Flowchart  | ç¬¬ 4.1.1 ç¯€ |
| æ¸¬è©¦é‡‘å­—å¡”åœ–                     | Diagram    | ç¬¬ 5.1.1 ç¯€ |
| å›æ¸¬æµç¨‹åœ–                       | Flowchart  | ç¬¬ 5.3.1 ç¯€ |
| éƒ¨ç½²æ¶æ§‹åœ–                       | C4 Model   | ç¬¬ 6.1 ç¯€   |
| å®Œæ•´é¡åˆ¥é—œä¿‚åœ–ï¼ˆUMLï¼‰            | Class      | ç¬¬ 7.1 ç¯€   |

---

## 2. ç³»çµ±æ¶æ§‹ç¸½è¦½

### 2.1 æ•´é«”æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "æ‡‰ç”¨å±¤ Application Layer"
        A1[æ¯æ—¥æ›´æ–°è…³æœ¬<br/>update_data.py]
        A2[ç­–ç•¥æƒæè…³æœ¬<br/>run_strategy.py]
        A3[å›æ¸¬å·¥å…·<br/>backtest.py]
    end

    subgraph "æ¥­å‹™é‚è¼¯å±¤ Business Logic Layer"
        B1[é¸è‚¡ç¯©é¸å™¨<br/>StockScreener]
        B2[å› å­è¨ˆç®—å¼•æ“<br/>FactorEngine]
        B3[é€šçŸ¥æœå‹™<br/>NotificationService]
    end

    subgraph "æ•¸æ“šè¨ªå•å±¤ Data Access Layer"
        C1[API å®¢æˆ¶ç«¯<br/>ShioajiClient]
        C2[Parquet ç®¡ç†å™¨<br/>ParquetManager]
        C3[æ•¸æ“šé©—è­‰å™¨<br/>DataValidator]
    end

    subgraph "å¤–éƒ¨æœå‹™ External Services"
        D1[(Shioaji API<br/>æ°¸è±é‡‘åˆ¸å•†)]
        D2[(TWSE<br/>è­‰äº¤æ‰€)]
        D3[(TDCC<br/>é›†ä¿çµç®—æ‰€)]
        D4[Line Notify]
        D5[Telegram Bot]
    end

    subgraph "æ•¸æ“šå„²å­˜å±¤ Data Storage Layer"
        E1[(/data/daily/<br/>æ™‚é–“åˆ†å€)]
        E2[(/data/history/<br/>å€‹è‚¡åˆ†å€)]
        E3[(/data/fundamentals/<br/>è²¡å ±æ•¸æ“š)]
        E4[(/reports/<br/>é¸è‚¡çµæœ)]
    end

    A1 --> B2
    A1 --> C1
    A1 --> E1

    A2 --> B1
    A2 --> B3

    B1 --> B2
    B1 --> C2

    B2 --> C2
    B2 --> E2
    B2 --> E3

    C1 --> D1
    C1 --> D2
    C1 --> D3

    B3 --> D4
    B3 --> D5

    C2 --> E1
    C2 --> E2
    C2 --> E3

    A2 --> E4
```

### 2.2 æ¨¡çµ„ä¾è³´é—œä¿‚åœ–

```mermaid
graph LR
    subgraph "æ ¸å¿ƒæ¨¡çµ„ src/"
        API[api_client.py<br/>ShioajiClient]
        ETL[etl_pipeline.py<br/>ETLPipeline]
        FACTOR[factors.py<br/>FactorEngine]
        SCREENER[screener.py<br/>StockScreener]
        UTILS[utils.py<br/>é€šç”¨å·¥å…·]
    end

    subgraph "è…³æœ¬å±¤ scripts/"
        UPDATE[update_data.py]
        STRATEGY[run_strategy.py]
        BACKTEST[backtest.py]
    end

    UPDATE --> API
    UPDATE --> ETL

    STRATEGY --> SCREENER
    STRATEGY --> FACTOR

    SCREENER --> FACTOR
    SCREENER --> ETL

    FACTOR --> ETL

    API --> UTILS
    ETL --> UTILS

    BACKTEST --> SCREENER
    BACKTEST --> ETL
```

### 2.3 æ•¸æ“šæµå‘åœ–ï¼ˆå®Œæ•´æ•¸æ“šç”Ÿå‘½é€±æœŸï¼‰

```mermaid
flowchart TD
    START([æ¯æ—¥ 15:00 å•Ÿå‹•])

    START --> FETCH_PRICE[æŠ“å–æ—¥è¡Œæƒ…<br/>Shioaji API]
    START --> FETCH_CHIPS[æŠ“å–ç±Œç¢¼æ•¸æ“š<br/>TWSE/TDCC]

    FETCH_PRICE --> VALIDATE1{æ•¸æ“šé©—è­‰<br/>å®Œæ•´æ€§/ç•°å¸¸å€¼}
    FETCH_CHIPS --> VALIDATE2{æ•¸æ“šé©—è­‰<br/>å®Œæ•´æ€§/ç•°å¸¸å€¼}

    VALIDATE1 -->|é€šé| WRITE_DAILY[å¯«å…¥æ™‚é–“åˆ†å€<br/>/data/daily/date=2024-01-01/]
    VALIDATE2 -->|é€šé| WRITE_CHIPS[å¯«å…¥ç±Œç¢¼æ•¸æ“š<br/>/data/chips/]

    VALIDATE1 -->|å¤±æ•—| ERROR_HANDLE[éŒ¯èª¤è™•ç†<br/>é€šçŸ¥ + æ—¥èªŒ]
    VALIDATE2 -->|å¤±æ•—| ERROR_HANDLE

    WRITE_DAILY --> ETL_PROCESS[ETL è½‰ç½®<br/>æ™‚é–“åˆ†å€ â†’ å€‹è‚¡åˆ†å€]
    ETL_PROCESS --> WRITE_HISTORY[å¯«å…¥å€‹è‚¡åˆ†å€<br/>/data/history/symbol=2330/]

    WRITE_HISTORY --> CALC_FACTORS[è¨ˆç®—å› å­<br/>ROE, EPS YoY, FCF...]
    WRITE_CHIPS --> CALC_FACTORS

    CALC_FACTORS --> SCREEN_L1[Layer 1: åŸºæœ¬é¢ç¯©é¸<br/>Top 500 â†’ Top 30]

    SCREEN_L1 --> SCREEN_L2[Layer 2: ç±Œç¢¼é¢é©—è­‰<br/>æ³•äººè²·è¶… + ç±Œç¢¼é›†ä¸­]

    SCREEN_L2 --> SCREEN_L3[Layer 3: æŠ€è¡“é¢ä½éš<br/>ä¹–é›¢ç‡ + KD/RSI]

    SCREEN_L3 --> OUTPUT_CSV[è¼¸å‡º CSV å ±è¡¨<br/>/reports/selections/]
    OUTPUT_CSV --> SEND_NOTIFY[ç™¼é€é€šçŸ¥<br/>Line/Telegram]

    SEND_NOTIFY --> END([å®Œæˆ])
    ERROR_HANDLE --> END
```

### 2.4 æŠ€è¡“æ£§èˆ‡è¨­è¨ˆæ¨¡å¼

#### æ ¸å¿ƒæŠ€è¡“æ£§

| å±¤ç´š           | æŠ€è¡“é¸å‹                        | ç‰ˆæœ¬è¦æ±‚    |
| -------------- | ------------------------------- | ----------- |
| ç¨‹å¼èªè¨€       | Python                          | 3.8+        |
| æ•¸æ“šè™•ç†       | pandas, numpy                   | Latest      |
| æ•¸æ“šå„²å­˜       | Apache Parquet (pyarrow)        | Latest      |
| API å®¢æˆ¶ç«¯     | Shioaji SDK                     | Latest      |
| æŠ€è¡“æŒ‡æ¨™è¨ˆç®—   | pandas-ta / ta-lib (optional)   | Latest      |
| æ’ç¨‹ç®¡ç†       | schedule                        | Latest      |
| é€šçŸ¥æ¨é€       | requests (Line), python-telegram-bot | Latest |
| æ¸¬è©¦æ¡†æ¶       | pytest, pytest-mock             | Latest      |
| æ—¥èªŒè¨˜éŒ„       | logging (å…§å»º)                  | -           |

#### è¨­è¨ˆæ¨¡å¼æ‡‰ç”¨

| æ¨¡çµ„               | è¨­è¨ˆæ¨¡å¼         | æ‡‰ç”¨èªªæ˜                                   |
| ------------------ | ---------------- | ------------------------------------------ |
| ShioajiClient      | Singleton        | ç¢ºä¿ API é€£ç·šå”¯ä¸€æ€§ï¼Œé¿å…é‡è¤‡èªè­‰         |
| FactorEngine       | Strategy         | ä¸åŒå› å­è¨ˆç®—ç­–ç•¥å¯æ’æ‹”                     |
| StockScreener      | Pipeline         | ä¸‰å±¤ç¯©é¸ä»¥ç®¡é“æ¨¡å¼ä¸²æ¥                     |
| ParquetManager     | Repository       | çµ±ä¸€çš„æ•¸æ“šè¨ªå•æ¥å£ï¼Œéš”é›¢å­˜å„²ç´°ç¯€           |
| NotificationService| Observer         | ç­–ç•¥çµæœè®ŠåŒ–æ™‚é€šçŸ¥è¨‚é–±è€…                   |
| DataValidator      | Chain of Responsibility | å¤šå€‹é©—è­‰è¦å‰‡éˆå¼èª¿ç”¨               |

---

## 3. æ ¸å¿ƒæ¨¡çµ„è¨­è¨ˆ

### 3.1 API å®¢æˆ¶ç«¯æ¨¡çµ„ (api_client.py)

#### 3.1.1 é¡åˆ¥æ¶æ§‹åœ–

```mermaid
classDiagram
    class ShioajiClient {
        -sj.Shioaji api
        -bool is_connected
        -threading.Lock lock
        +__init__(api_key, secret_key)
        +bool connect()
        +void disconnect()
        +bool login()
        +DataFrame get_stock_snapshot(symbol)
        +DataFrame get_historical_data(symbol, start, end)
        +DataFrame get_institutional_trades(date)
        +__enter__()
        +__exit__()
    }

    class ConnectionManager {
        -int retry_count
        -int retry_delay
        +bool establish_connection()
        +void handle_disconnect()
        +bool check_connection_health()
    }

    class RateLimiter {
        -int max_requests
        -int time_window
        -list request_timestamps
        +bool allow_request()
        +void wait_if_needed()
    }

    ShioajiClient *-- ConnectionManager
    ShioajiClient *-- RateLimiter
```

#### 3.1.2 é€£ç·šèˆ‡èªè­‰æ™‚åºåœ–

```mermaid
sequenceDiagram
    participant Script as æ›´æ–°è…³æœ¬
    participant Client as ShioajiClient
    participant ConnMgr as ConnectionManager
    participant API as Shioaji API
    participant Limiter as RateLimiter

    Script->>Client: connect()
    Client->>ConnMgr: establish_connection()

    ConnMgr->>API: api.login(api_key, secret_key)
    API-->>ConnMgr: èªè­‰æˆåŠŸ / å¤±æ•—

    alt èªè­‰å¤±æ•—
        ConnMgr->>ConnMgr: retry (æœ€å¤š 3 æ¬¡)
        ConnMgr->>API: api.login()
        API-->>ConnMgr: é‡è©¦çµæœ
    end

    ConnMgr-->>Client: é€£ç·šç‹€æ…‹
    Client-->>Script: True / False

    Script->>Client: get_historical_data(symbol="2330")
    Client->>Limiter: allow_request()

    alt è¶…éé€Ÿç‡é™åˆ¶
        Limiter->>Limiter: wait_if_needed()
    end

    Limiter-->>Client: å…è¨±è«‹æ±‚
    Client->>API: fetch_data()
    API-->>Client: DataFrame
    Client-->>Script: æ­·å²æ•¸æ“š
```

#### 3.1.3 å½ä»£ç¢¼ï¼šæ ¸å¿ƒæ–¹æ³•å¯¦ä½œ

```python
# å½ä»£ç¢¼ï¼šShioajiClient æ ¸å¿ƒå¯¦ä½œ

class ShioajiClient:
    """
    Shioaji API å®¢æˆ¶ç«¯å°è£

    è¨­è¨ˆæ¨¡å¼ï¼šSingleton + Context Manager
    è·è²¬ï¼šAPI é€£ç·šç®¡ç†ã€æ•¸æ“šæŠ“å–ã€éŒ¯èª¤è™•ç†
    """

    _instance = None  # Singleton å¯¦ä¾‹

    def __new__(cls, *args, **kwargs):
        """ç¢ºä¿å–®ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, api_key: str, secret_key: str):
        """
        åˆå§‹åŒ– API å®¢æˆ¶ç«¯

        åƒæ•¸:
            api_key: æ°¸è±é‡‘ API é‡‘é‘°
            secret_key: æ°¸è±é‡‘å¯†é‘°
        """
        if not hasattr(self, 'initialized'):
            self.api_key = api_key
            self.secret_key = secret_key
            self.api = None
            self.is_connected = False
            self.lock = threading.Lock()
            self.rate_limiter = RateLimiter(max_requests=60, time_window=60)
            self.initialized = True

    def connect(self) -> bool:
        """
        å»ºç«‹ API é€£ç·š

        è¿”å›:
            bool: é€£ç·šæˆåŠŸè¿”å› True

        ç•°å¸¸:
            ConnectionError: é€£ç·šå¤±æ•—æ™‚æ‹‹å‡º
        """
        with self.lock:
            if self.is_connected:
                return True

            try:
                # æ­¥é©Ÿ 1: åˆå§‹åŒ– Shioaji API
                self.api = sj.Shioaji()

                # æ­¥é©Ÿ 2: ç™»å…¥èªè­‰ï¼ˆæœ€å¤šé‡è©¦ 3 æ¬¡ï¼‰
                for attempt in range(3):
                    try:
                        self.api.login(
                            api_key=self.api_key,
                            secret_key=self.secret_key
                        )
                        self.is_connected = True
                        logging.info("Shioaji API é€£ç·šæˆåŠŸ")
                        return True
                    except Exception as e:
                        logging.warning(f"ç™»å…¥å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
                        if attempt < 2:
                            time.sleep(5)  # ç­‰å¾… 5 ç§’å¾Œé‡è©¦
                        else:
                            raise ConnectionError(f"API ç™»å…¥å¤±æ•—: {e}")

            except Exception as e:
                logging.error(f"API é€£ç·šå¤±æ•—: {e}")
                self.is_connected = False
                raise

    def get_historical_data(
        self,
        symbol: str,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        å–å¾—æ­·å² K ç·šæ•¸æ“š

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚ "2330"ï¼‰
            start_date: é–‹å§‹æ—¥æœŸ ("2024-01-01")
            end_date: çµæŸæ—¥æœŸ ("2024-12-31")

        è¿”å›:
            DataFrame: åŒ…å« OHLCV æ•¸æ“š

        æ¬„ä½:
            date, open, high, low, close, volume
        """
        # æ­¥é©Ÿ 1: æª¢æŸ¥é€£ç·šç‹€æ…‹
        if not self.is_connected:
            self.connect()

        # æ­¥é©Ÿ 2: é€Ÿç‡é™åˆ¶æª¢æŸ¥
        self.rate_limiter.wait_if_needed()

        # æ­¥é©Ÿ 3: å‘¼å« API
        try:
            contract = self.api.Contracts.Stocks[symbol]
            kbars = self.api.kbars(
                contract=contract,
                start=start_date,
                end=end_date
            )

            # æ­¥é©Ÿ 4: è½‰æ›ç‚º DataFrame
            df = pd.DataFrame({
                'date': kbars.ts,
                'open': kbars.Open,
                'high': kbars.High,
                'low': kbars.Low,
                'close': kbars.Close,
                'volume': kbars.Volume
            })

            return df

        except Exception as e:
            logging.error(f"æŠ“å– {symbol} æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            raise

    def get_institutional_trades(self, date: str) -> pd.DataFrame:
        """
        å–å¾—ä¸‰å¤§æ³•äººè²·è³£è¶…æ•¸æ“š

        åƒæ•¸:
            date: æ—¥æœŸ ("2024-01-01")

        è¿”å›:
            DataFrame: æ³•äººè²·è³£è¶…æ•¸æ“š

        æ¬„ä½:
            symbol, foreign_buy, foreign_sell, foreign_net,
            trust_buy, trust_sell, trust_net,
            dealer_buy, dealer_sell, dealer_net
        """
        # å¯¦ä½œé‚è¼¯ï¼ˆé¡ä¼¼ä¸Šè¿°ï¼‰
        pass

    def __enter__(self):
        """Context Manager é€²å…¥"""
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context Manager é€€å‡º"""
        if self.api:
            self.api.logout()
            self.is_connected = False
```

#### 3.1.4 éŒ¯èª¤è™•ç†æ©Ÿåˆ¶

```python
# å½ä»£ç¢¼ï¼šéŒ¯èª¤è™•ç†èˆ‡é‡è©¦é‚è¼¯

class APIErrorHandler:
    """API éŒ¯èª¤è™•ç†å™¨"""

    @staticmethod
    def retry_on_failure(max_retries=3, delay=5):
        """
        å¤±æ•—é‡è©¦è£é£¾å™¨

        ä½¿ç”¨ç¯„ä¾‹:
            @APIErrorHandler.retry_on_failure(max_retries=3)
            def fetch_data():
                return api.get_data()
        """
        def decorator(func):
            def wrapper(*args, **kwargs):
                for attempt in range(max_retries):
                    try:
                        return func(*args, **kwargs)
                    except (ConnectionError, TimeoutError) as e:
                        if attempt < max_retries - 1:
                            logging.warning(
                                f"{func.__name__} å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}"
                            )
                            time.sleep(delay * (attempt + 1))  # æŒ‡æ•¸é€€é¿
                        else:
                            logging.error(f"{func.__name__} æœ€çµ‚å¤±æ•—: {e}")
                            raise
            return wrapper
        return decorator
```

---

### 3.2 å› å­è¨ˆç®—å¼•æ“ (factors.py)

#### 3.2.1 å› å­è¨ˆç®—æµç¨‹åœ–

```mermaid
flowchart TD
    START([é–‹å§‹è¨ˆç®—å› å­])

    LOAD_FIN[è¼‰å…¥è²¡å ±æ•¸æ“š<br/>/data/fundamentals/]
    LOAD_PRICE[è¼‰å…¥åƒ¹æ ¼æ•¸æ“š<br/>/data/history/]
    LOAD_CHIPS[è¼‰å…¥ç±Œç¢¼æ•¸æ“š<br/>/data/chips/]

    START --> LOAD_FIN
    START --> LOAD_PRICE
    START --> LOAD_CHIPS

    LOAD_FIN --> CALC_ROE[è¨ˆç®— ROE<br/>ç¨…å¾Œæ·¨åˆ© Ã· å¹³å‡è‚¡æ±æ¬Šç›Š]
    LOAD_FIN --> CALC_MARGIN[è¨ˆç®—æ¯›åˆ©ç‡è¶¨å‹¢<br/>è¿‘ 3 å­£è®ŠåŒ–]
    LOAD_FIN --> CALC_DEBT[è¨ˆç®—è² å‚µæ¯”ç‡<br/>ç¸½è² å‚µ Ã· ç¸½è³‡ç”¢]
    LOAD_FIN --> CALC_FCF[è¨ˆç®—è‡ªç”±ç¾é‡‘æµ<br/>ç‡Ÿæ¥­ç¾é‡‘æµ - è³‡æœ¬æ”¯å‡º]
    LOAD_FIN --> CALC_REV_YOY[è¨ˆç®—ç‡Ÿæ”¶ YoY<br/>(æœ¬æœŸ - å»å¹´åŒæœŸ) Ã· å»å¹´åŒæœŸ]
    LOAD_FIN --> CALC_EPS_YoY[è¨ˆç®— EPS YoY<br/>(æœ¬æœŸ - å»å¹´åŒæœŸ) Ã· å»å¹´åŒæœŸ]

    LOAD_PRICE --> CALC_PE[è¨ˆç®— P/E vs æ­·å²<br/>ç•¶å‰ PE Ã· æ­·å²ä¸­ä½æ•¸]

    CALC_ROE --> SCORE_ROE[è©•åˆ† ROE<br/>1-5 åˆ†]
    CALC_MARGIN --> SCORE_MARGIN[è©•åˆ†æ¯›åˆ©ç‡è¶¨å‹¢<br/>1-5 åˆ†]
    CALC_DEBT --> SCORE_DEBT[è©•åˆ†è² å‚µæ¯”ç‡<br/>1-5 åˆ†]
    CALC_FCF --> SCORE_FCF[è©•åˆ† FCF<br/>1-5 åˆ†]
    CALC_REV_YOY --> SCORE_REV[è©•åˆ†ç‡Ÿæ”¶ YoY<br/>1-5 åˆ†]
    CALC_EPS_YoY --> SCORE_EPS[è©•åˆ† EPS YoY<br/>1-5 åˆ†]
    CALC_PE --> SCORE_PE[è©•åˆ† P/E<br/>1-5 åˆ†]

    SCORE_ROE --> AGGREGATE[åŠ æ¬Šèšåˆ<br/>ROEÃ—10% + æ¯›åˆ©ç‡Ã—5% + ...]
    SCORE_MARGIN --> AGGREGATE
    SCORE_DEBT --> AGGREGATE
    SCORE_FCF --> AGGREGATE
    SCORE_REV --> AGGREGATE
    SCORE_EPS --> AGGREGATE
    SCORE_PE --> AGGREGATE

    AGGREGATE --> TOTAL_SCORE[åŸºæœ¬é¢ç¸½åˆ†<br/>0-200 åˆ†]
    TOTAL_SCORE --> CACHE[å¯«å…¥ç·©å­˜<br/>Redis / è¨˜æ†¶é«”]
    CACHE --> END([å®Œæˆ])
```

#### 3.2.2 ä¸ƒå› å­å¯¦ä½œæ¶æ§‹

```python
# å½ä»£ç¢¼ï¼šFactorEngine æ¶æ§‹

class FactorEngine:
    """
    å› å­è¨ˆç®—å¼•æ“

    è·è²¬ï¼šè¨ˆç®—æ‰€æœ‰åŸºæœ¬é¢ã€ç±Œç¢¼é¢ã€æŠ€è¡“é¢å› å­
    è¨­è¨ˆæ¨¡å¼ï¼šStrategy Patternï¼ˆä¸åŒå› å­ç­–ç•¥å¯æ’æ‹”ï¼‰
    """

    def __init__(self, data_manager: ParquetManager):
        """
        åˆå§‹åŒ–å› å­å¼•æ“

        åƒæ•¸:
            data_manager: æ•¸æ“šç®¡ç†å™¨
        """
        self.data_manager = data_manager
        self.cache = {}  # å› å­ç·©å­˜

        # è¨»å†Šå› å­è¨ˆç®—ç­–ç•¥
        self.fundamental_factors = {
            'roe': self._calculate_roe,
            'gross_margin_trend': self._calculate_gross_margin_trend,
            'debt_ratio': self._calculate_debt_ratio,
            'fcf': self._calculate_fcf,
            'revenue_yoy': self._calculate_revenue_yoy,
            'eps_yoy': self._calculate_eps_yoy,
            'pe_relative': self._calculate_pe_relative
        }

    def calculate_fundamental_score(self, symbol: str) -> float:
        """
        è¨ˆç®—åŸºæœ¬é¢ç¶œåˆå¾—åˆ†

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼

        è¿”å›:
            float: 0-200 åˆ†
        """
        # æ­¥é©Ÿ 1: æª¢æŸ¥ç·©å­˜
        cache_key = f"{symbol}_fundamental_{date.today()}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # æ­¥é©Ÿ 2: è¨ˆç®—å„å› å­åˆ†æ•¸
        scores = {}
        for factor_name, calc_func in self.fundamental_factors.items():
            raw_value = calc_func(symbol)
            scores[factor_name] = self._score_factor(factor_name, raw_value)

        # æ­¥é©Ÿ 3: åŠ æ¬Šèšåˆ
        total_score = (
            scores['roe'] * 0.10 +
            scores['gross_margin_trend'] * 0.05 +
            scores['debt_ratio'] * 0.05 +
            scores['fcf'] * 0.05 +
            scores['revenue_yoy'] * 0.05 +
            scores['eps_yoy'] * 0.05 +
            scores['pe_relative'] * 0.05
        ) * 100

        # æ­¥é©Ÿ 4: å¯«å…¥ç·©å­˜
        self.cache[cache_key] = total_score

        return total_score

    def _calculate_roe(self, symbol: str) -> float:
        """
        è¨ˆç®— ROEï¼ˆè¿‘å››å­£å¹³å‡ï¼‰

        å…¬å¼:
            ROE = ç¨…å¾Œæ·¨åˆ© Ã· å¹³å‡è‚¡æ±æ¬Šç›Š Ã— 100%

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼

        è¿”å›:
            float: ROE ç™¾åˆ†æ¯”
        """
        # æ­¥é©Ÿ 1: è¼‰å…¥è²¡å ±æ•¸æ“š
        financials = self.data_manager.read_parquet(
            f'/data/fundamentals/quarterly/{symbol}.parquet'
        )

        # æ­¥é©Ÿ 2: å–å¾—è¿‘å››å­£æ•¸æ“š
        recent_4q = financials.tail(4)

        # æ­¥é©Ÿ 3: è¨ˆç®—
        net_income = recent_4q['net_income'].sum()  # è¿‘å››å­£ç¨…å¾Œæ·¨åˆ©ç¸½å’Œ
        avg_equity = recent_4q['equity'].mean()     # å¹³å‡è‚¡æ±æ¬Šç›Š

        if avg_equity == 0:
            return 0.0

        roe = (net_income / avg_equity) * 100

        return roe

    def _calculate_eps_yoy(self, symbol: str) -> float:
        """
        è¨ˆç®— EPS å¹´å¢ç‡

        å…¬å¼:
            EPS YoY = (æœ¬å­£ EPS - å»å¹´åŒå­£ EPS) Ã· |å»å¹´åŒå­£ EPS| Ã— 100%

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼

        è¿”å›:
            float: EPS YoY ç™¾åˆ†æ¯”
        """
        financials = self.data_manager.read_parquet(
            f'/data/fundamentals/quarterly/{symbol}.parquet'
        )

        # å–å¾—æœ€è¿‘ä¸€å­£èˆ‡å»å¹´åŒå­£çš„ EPS
        latest_eps = financials.iloc[-1]['eps']
        yoy_eps = financials.iloc[-5]['eps']  # å»å¹´åŒå­£ï¼ˆ4 å­£å‰ï¼‰

        if abs(yoy_eps) < 0.01:  # é¿å…é™¤ä»¥é›¶
            return 0.0

        eps_yoy = ((latest_eps - yoy_eps) / abs(yoy_eps)) * 100

        return eps_yoy

    def _calculate_fcf(self, symbol: str) -> float:
        """
        è¨ˆç®—è‡ªç”±ç¾é‡‘æµ

        å…¬å¼:
            FCF = ç‡Ÿæ¥­ç¾é‡‘æµ - è³‡æœ¬æ”¯å‡º

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼

        è¿”å›:
            float: FCF é‡‘é¡
        """
        financials = self.data_manager.read_parquet(
            f'/data/fundamentals/quarterly/{symbol}.parquet'
        )

        # å–å¾—æœ€è¿‘ä¸€å­£æ•¸æ“š
        latest = financials.iloc[-1]
        operating_cf = latest['operating_cash_flow']
        capex = latest['capital_expenditure']

        fcf = operating_cf - capex

        return fcf

    def _score_factor(self, factor_name: str, raw_value: float) -> int:
        """
        å°‡å› å­åŸå§‹å€¼è½‰æ›ç‚º 1-5 åˆ†

        åƒæ•¸:
            factor_name: å› å­åç¨±
            raw_value: åŸå§‹æ•¸å€¼

        è¿”å›:
            int: 1-5 åˆ†
        """
        # è©•åˆ†è¦å‰‡ï¼ˆæ ¹æ“š Overview.md çš„æ¨™æº–ï¼‰
        scoring_rules = {
            'roe': [
                (15, 5), (10, 4), (5, 3), (0, 2), (-float('inf'), 1)
            ],
            'eps_yoy': [
                (20, 5), (10, 4), (0, 3), (-20, 2), (-float('inf'), 1)
            ],
            'gross_margin_trend': [
                (2, 5), (0, 4), (-2, 3), (-5, 2), (-float('inf'), 1)
            ],
            'debt_ratio': [
                (30, 5), (50, 4), (70, 3), (80, 2), (100, 1)
            ],
            'fcf': [
                (10000, 5), (5000, 4), (0, 3), (-5000, 2), (-float('inf'), 1)
            ],
            'revenue_yoy': [
                (15, 5), (5, 4), (0, 3), (-5, 2), (-float('inf'), 1)
            ],
            'pe_relative': [
                (0.7, 5), (0.9, 4), (1.0, 3), (1.2, 2), (float('inf'), 1)
            ]
        }

        rules = scoring_rules.get(factor_name, [])
        for threshold, score in rules:
            if factor_name == 'debt_ratio':
                # è² å‚µæ¯”ç‡è¶Šä½è¶Šå¥½
                if raw_value <= threshold:
                    return score
            elif factor_name == 'pe_relative':
                # PE ç›¸å°å€¼è¶Šä½è¶Šå¥½
                if raw_value <= threshold:
                    return score
            else:
                # å…¶ä»–å› å­è¶Šé«˜è¶Šå¥½
                if raw_value >= threshold:
                    return score

        return 1  # é»˜èªæœ€ä½åˆ†
```

---

### 3.3 é¸è‚¡ç¯©é¸å™¨ (screener.py)

#### 3.3.1 ä¸‰å±¤ç¯©é¸æµç¨‹åœ–

```mermaid
flowchart TD
    START([é–‹å§‹é¸è‚¡<br/>Top 500])

    START --> LAYER1_START[Layer 1: åŸºæœ¬é¢ç¯©é¸]

    LAYER1_START --> CALC_FUND[è¨ˆç®—åŸºæœ¬é¢å¾—åˆ†<br/>7 å› å­åŠ æ¬Š]
    CALC_FUND --> SORT_FUND[æ’åº<br/>åŸºæœ¬é¢å¾—åˆ† DESC]
    SORT_FUND --> SELECT_TOP30[é¸å– Top 30]

    SELECT_TOP30 --> LAYER2_START[Layer 2: ç±Œç¢¼é¢é©—è­‰]

    LAYER2_START --> CHECK_INST{æ³•äººè²·è³£è¶…<br/>è¿‘ 5 æ—¥åˆè¨ˆ > 0?}
    CHECK_INST -->|å¦| REJECT1[å‰”é™¤]
    CHECK_INST -->|æ˜¯| CHECK_HOLDER{å¤§æˆ¶æŒè‚¡<br/>è¼ƒä¸Šé€±å¢åŠ ?}

    CHECK_HOLDER -->|å¦| REJECT2[å‰”é™¤]
    CHECK_HOLDER -->|æ˜¯| PASS_LAYER2[é€šéç±Œç¢¼é¢]

    PASS_LAYER2 --> LAYER3_START[Layer 3: æŠ€è¡“é¢ä½éš]

    LAYER3_START --> CALC_BIAS[è¨ˆç®—ä¹–é›¢ç‡<br/>Price vs MA60]
    CALC_BIAS --> CHECK_BIAS{ä¹–é›¢ç‡ 0-10%<br/>ä½ä½éš?}

    CHECK_BIAS -->|å¦| LABEL_MEDIUM[æ¨™è¨˜ç‚ºä¸­/é«˜ä½éš<br/>é™ä½å„ªå…ˆç´š]
    CHECK_BIAS -->|æ˜¯| CALC_KD[è¨ˆç®— KD æŒ‡æ¨™]

    CALC_KD --> CHECK_KD{KD é»ƒé‡‘äº¤å‰<br/>K > D?}
    CHECK_KD -->|æ˜¯| SIGNAL_STRONG[**å¼·åŠ›è²·é€²**]
    CHECK_KD -->|å¦| SIGNAL_NORMAL[è§€å¯Ÿ]

    LABEL_MEDIUM --> SIGNAL_HOLD[æŒæœ‰/æ¸›ç¢¼]

    SIGNAL_STRONG --> OUTPUT[è¼¸å‡ºé¸è‚¡çµæœ<br/>CSV + é€šçŸ¥]
    SIGNAL_NORMAL --> OUTPUT
    SIGNAL_HOLD --> OUTPUT

    REJECT1 --> END([å®Œæˆ])
    REJECT2 --> END
    OUTPUT --> END
```

#### 3.3.2 ç‹€æ…‹æ©Ÿåœ–ï¼šç¯©é¸é‚è¼¯

```mermaid
stateDiagram-v2
    [*] --> Pending: é€²å…¥å€™é¸æ±  (Top 500)

    Pending --> FundamentalEval: é–‹å§‹è©•ä¼°

    FundamentalEval --> Top30: åŸºæœ¬é¢å¾—åˆ† Top 30
    FundamentalEval --> Rejected: åŸºæœ¬é¢å¾—åˆ†ä½

    Top30 --> ChipEval: é€²å…¥ç±Œç¢¼é¢æª¢æŸ¥

    ChipEval --> ChipPass: æ³•äººè²·è¶… + å¤§æˆ¶å¢æŒ
    ChipEval --> ChipWatch: æ³•äººè²·è¶…ä½†å¤§æˆ¶æ¸›æŒ
    ChipEval --> Rejected: æ³•äººè³£è¶…

    ChipPass --> TechEval: é€²å…¥æŠ€è¡“é¢è©•ä¼°
    ChipWatch --> TechEval: é€²å…¥æŠ€è¡“é¢è©•ä¼°

    TechEval --> StrongBuy: ä½ä½éš + KD é»ƒé‡‘äº¤å‰
    TechEval --> Buy: ä½ä½éšä½† KD æœªäº¤å‰
    TechEval --> Hold: ä¸­ä½éš
    TechEval --> Reduce: é«˜ä½éš

    StrongBuy --> [*]: è¼¸å‡ºå¼·åŠ›è²·é€²è¨Šè™Ÿ
    Buy --> [*]: è¼¸å‡ºè²·é€²è¨Šè™Ÿ
    Hold --> [*]: è¼¸å‡ºæŒæœ‰è¨Šè™Ÿ
    Reduce --> [*]: è¼¸å‡ºæ¸›ç¢¼è¨Šè™Ÿ
    Rejected --> [*]: ä¸è¼¸å‡º
```

#### 3.3.3 å½ä»£ç¢¼ï¼šç¶œåˆè©•åˆ†æ¨¡å‹

```python
# å½ä»£ç¢¼ï¼šStockScreener å®Œæ•´å¯¦ä½œ

class StockScreener:
    """
    é¸è‚¡ç¯©é¸å™¨

    è·è²¬ï¼šä¸‰å±¤ç¯©é¸é‚è¼¯ã€ç¶œåˆè©•åˆ†ã€è¨Šè™Ÿç”Ÿæˆ
    è¨­è¨ˆæ¨¡å¼ï¼šPipeline Pattern
    """

    def __init__(
        self,
        factor_engine: FactorEngine,
        data_manager: ParquetManager
    ):
        self.factor_engine = factor_engine
        self.data_manager = data_manager
        self.logger = logging.getLogger(__name__)

    def screen_stocks(self, universe: List[str]) -> pd.DataFrame:
        """
        åŸ·è¡Œå®Œæ•´é¸è‚¡æµç¨‹

        åƒæ•¸:
            universe: è‚¡ç¥¨æ± ï¼ˆå¦‚ Top 500 ä»£ç¢¼åˆ—è¡¨ï¼‰

        è¿”å›:
            DataFrame: é¸è‚¡çµæœ

        æ¬„ä½:
            symbol, name, fundamental_score, chip_status,
            tech_position, signal, bias_60, kd_cross
        """
        self.logger.info(f"é–‹å§‹é¸è‚¡ï¼Œå€™é¸æ± : {len(universe)} æª”")

        # Layer 1: åŸºæœ¬é¢ç¯©é¸
        top_30 = self._layer1_fundamental_screen(universe)
        self.logger.info(f"Layer 1 å®Œæˆï¼Œç¯©é¸å‡º {len(top_30)} æª”")

        # Layer 2: ç±Œç¢¼é¢é©—è­‰
        chip_passed = self._layer2_chip_filter(top_30)
        self.logger.info(f"Layer 2 å®Œæˆï¼Œé€šé {len(chip_passed)} æª”")

        # Layer 3: æŠ€è¡“é¢ä½éšåˆ¤æ–·
        final_results = self._layer3_technical_position(chip_passed)
        self.logger.info(f"Layer 3 å®Œæˆï¼Œæœ€çµ‚ {len(final_results)} æª”")

        return final_results

    def _layer1_fundamental_screen(
        self,
        universe: List[str]
    ) -> pd.DataFrame:
        """
        Layer 1: åŸºæœ¬é¢ç¯©é¸

        é‚è¼¯:
            1. è¨ˆç®— 7 å› å­ç¶œåˆå¾—åˆ†
            2. æ’åºä¸¦é¸å– Top 30

        åƒæ•¸:
            universe: è‚¡ç¥¨æ± 

        è¿”å›:
            DataFrame: Top 30 è‚¡ç¥¨
        """
        scores = []

        for symbol in universe:
            try:
                # è¨ˆç®—åŸºæœ¬é¢å¾—åˆ†
                fund_score = self.factor_engine.calculate_fundamental_score(
                    symbol
                )
                scores.append({
                    'symbol': symbol,
                    'fundamental_score': fund_score
                })
            except Exception as e:
                self.logger.warning(f"è¨ˆç®— {symbol} åŸºæœ¬é¢å¤±æ•—: {e}")
                continue

        # è½‰ç‚º DataFrame ä¸¦æ’åº
        df = pd.DataFrame(scores)
        df = df.sort_values('fundamental_score', ascending=False)

        # å– Top 30
        top_30 = df.head(30).copy()

        return top_30

    def _layer2_chip_filter(
        self,
        candidates: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Layer 2: ç±Œç¢¼é¢éæ¿¾

        æ¢ä»¶:
            âœ… è¿‘ 5 æ—¥ä¸‰å¤§æ³•äººåˆè¨ˆè²·è¶… > 0
            âœ… æœ€è¿‘ä¸€é€±å¤§æˆ¶æŒè‚¡æ¯”ä¾‹è¼ƒä¸Šé€±å¢åŠ 

        åƒæ•¸:
            candidates: Layer 1 é€šéçš„è‚¡ç¥¨

        è¿”å›:
            DataFrame: é€šéç±Œç¢¼é¢çš„è‚¡ç¥¨
        """
        passed = []

        for _, row in candidates.iterrows():
            symbol = row['symbol']

            try:
                # æª¢æŸ¥ 1: æ³•äººè²·è³£è¶…
                inst_net = self._check_institutional_net(symbol, days=5)

                # æª¢æŸ¥ 2: å¤§æˆ¶æŒè‚¡è®ŠåŒ–
                major_holder_change = self._check_major_holder_change(symbol)

                # åˆ¤å®š
                if inst_net > 0 and major_holder_change > 0:
                    row['chip_status'] = 'pass'
                    row['inst_net_5d'] = inst_net
                    row['major_holder_chg'] = major_holder_change
                    passed.append(row)

            except Exception as e:
                self.logger.warning(f"æª¢æŸ¥ {symbol} ç±Œç¢¼é¢å¤±æ•—: {e}")
                continue

        return pd.DataFrame(passed)

    def _layer3_technical_position(
        self,
        candidates: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Layer 3: æŠ€è¡“é¢ä½éšåˆ¤æ–·

        æŒ‡æ¨™:
            - ä¹–é›¢ç‡ï¼ˆBias 60ï¼‰
            - KD é»ƒé‡‘äº¤å‰
            - RSI ä½ç½®

        è¨Šè™Ÿç”Ÿæˆ:
            - ä¹–é›¢ç‡ 0-10% + KD é»ƒé‡‘äº¤å‰ â†’ å¼·åŠ›è²·é€²
            - ä¹–é›¢ç‡ 10-20% â†’ é †å‹¢åŠ ç¢¼
            - ä¹–é›¢ç‡ > 20% â†’ æ¸›ç¢¼

        åƒæ•¸:
            candidates: Layer 2 é€šéçš„è‚¡ç¥¨

        è¿”å›:
            DataFrame: æœ€çµ‚é¸è‚¡çµæœï¼ˆå«è¨Šè™Ÿï¼‰
        """
        results = []

        for _, row in candidates.iterrows():
            symbol = row['symbol']

            try:
                # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
                bias_60 = self._calculate_bias(symbol, ma_period=60)
                kd_cross = self._check_kd_cross(symbol)
                rsi_14 = self._calculate_rsi(symbol, period=14)

                # è¨Šè™Ÿåˆ¤æ–·
                if 0 <= bias_60 <= 10 and kd_cross == 'golden':
                    signal = 'å¼·åŠ›è²·é€²'
                elif 10 < bias_60 <= 20:
                    signal = 'é †å‹¢åŠ ç¢¼'
                elif bias_60 > 20:
                    signal = 'æ¸›ç¢¼'
                else:
                    signal = 'è§€æœ›'

                # è£œå……æŠ€è¡“é¢æ¬„ä½
                row['bias_60'] = bias_60
                row['kd_cross'] = kd_cross
                row['rsi_14'] = rsi_14
                row['signal'] = signal

                results.append(row)

            except Exception as e:
                self.logger.warning(f"è¨ˆç®— {symbol} æŠ€è¡“é¢å¤±æ•—: {e}")
                continue

        return pd.DataFrame(results)

    def _calculate_bias(self, symbol: str, ma_period: int = 60) -> float:
        """
        è¨ˆç®—ä¹–é›¢ç‡

        å…¬å¼:
            Bias = (Price - MA) / MA Ã— 100%

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            ma_period: å‡ç·šé€±æœŸ

        è¿”å›:
            float: ä¹–é›¢ç‡ç™¾åˆ†æ¯”
        """
        # è¼‰å…¥åƒ¹æ ¼æ•¸æ“š
        price_data = self.data_manager.read_parquet(
            f'/data/history/symbol={symbol}/data.parquet'
        )

        # è¨ˆç®—ç§»å‹•å¹³å‡ç·š
        price_data[f'ma{ma_period}'] = price_data['close'].rolling(
            window=ma_period
        ).mean()

        # å–å¾—æœ€æ–°æ•¸æ“š
        latest = price_data.iloc[-1]
        price = latest['close']
        ma = latest[f'ma{ma_period}']

        if ma == 0:
            return 0.0

        bias = ((price - ma) / ma) * 100

        return bias

    def _check_kd_cross(self, symbol: str) -> str:
        """
        æª¢æŸ¥ KD æŒ‡æ¨™äº¤å‰ç‹€æ…‹

        è¿”å›:
            str: 'golden' (é»ƒé‡‘äº¤å‰), 'death' (æ­»äº¡äº¤å‰), 'none' (ç„¡äº¤å‰)
        """
        price_data = self.data_manager.read_parquet(
            f'/data/history/symbol={symbol}/data.parquet'
        )

        # è¨ˆç®— KD æŒ‡æ¨™
        # (å¯¦éš›å¯¦ä½œæ‡‰ä½¿ç”¨ pandas-ta æˆ– ta-lib)
        k_values = price_data['k']  # å‡è¨­å·²è¨ˆç®—
        d_values = price_data['d']

        # å–å¾—æœ€è¿‘å…©ç­†æ•¸æ“š
        k_today, k_yesterday = k_values.iloc[-1], k_values.iloc[-2]
        d_today, d_yesterday = d_values.iloc[-1], d_values.iloc[-2]

        # åˆ¤æ–·äº¤å‰
        if k_yesterday <= d_yesterday and k_today > d_today:
            return 'golden'
        elif k_yesterday >= d_yesterday and k_today < d_today:
            return 'death'
        else:
            return 'none'

    def _check_institutional_net(self, symbol: str, days: int = 5) -> float:
        """
        è¨ˆç®—è¿‘ N æ—¥æ³•äººè²·è³£è¶…åˆè¨ˆ

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            days: å¤©æ•¸

        è¿”å›:
            float: æ³•äººè²·è³£è¶…åˆè¨ˆï¼ˆå¼µæ•¸ï¼‰
        """
        inst_data = self.data_manager.read_parquet(
            '/data/chips/institutional/data.parquet'
        )

        # ç¯©é¸è©²è‚¡ç¥¨è¿‘ N æ—¥æ•¸æ“š
        symbol_data = inst_data[inst_data['symbol'] == symbol].tail(days)

        # è¨ˆç®—åˆè¨ˆ
        total_net = symbol_data['foreign_net'].sum() + \
                    symbol_data['trust_net'].sum() + \
                    symbol_data['dealer_net'].sum()

        return total_net
```

---

### 3.4 æ•¸æ“šç®¡é“ (etl_pipeline.py)

#### 3.4.1 ETL æµç¨‹åœ–

```mermaid
flowchart TD
    START([é–‹å§‹ ETL è½‰ç½®])

    START --> READ_DAILY[è®€å–æ™‚é–“åˆ†å€<br/>/data/daily/date=*/]
    READ_DAILY --> LOAD_ALL[è¼‰å…¥æ‰€æœ‰æ—¥æœŸæ•¸æ“š<br/>åˆä½µç‚ºå–®ä¸€ DataFrame]

    LOAD_ALL --> GROUP_BY[ä¾ symbol åˆ†çµ„<br/>pandas.groupby('symbol')]

    GROUP_BY --> ITERATE{éæ­·æ¯æª”è‚¡ç¥¨}

    ITERATE --> EXTRACT[æå–è©²è‚¡ç¥¨<br/>æ‰€æœ‰æ­·å²æ•¸æ“š]
    EXTRACT --> SORT[ä¾æ—¥æœŸæ’åº<br/>sort_values('date')]

    SORT --> ENRICH[è£œå……æŠ€è¡“æŒ‡æ¨™<br/>MA, RSI, KD, MACD]

    ENRICH --> WRITE_SYMBOL[å¯«å…¥å€‹è‚¡åˆ†å€<br/>/data/history/symbol=*/]

    WRITE_SYMBOL --> NEXT{é‚„æœ‰è‚¡ç¥¨?}
    NEXT -->|æ˜¯| ITERATE
    NEXT -->|å¦| CLEANUP

    CLEANUP[æ¸…ç†æš«å­˜æª”<br/>åˆªé™¤è‡¨æ™‚æ–‡ä»¶]
    CLEANUP --> LOG[è¨˜éŒ„æ—¥èªŒ<br/>è½‰ç½®å®Œæˆ + çµ±è¨ˆä¿¡æ¯]

    LOG --> END([å®Œæˆ])
```

#### 3.4.2 å½ä»£ç¢¼ï¼šParquet ç®¡ç†å™¨

```python
# å½ä»£ç¢¼ï¼šParquetManager æ•¸æ“šè¨ªå•å±¤

class ParquetManager:
    """
    Parquet æ•¸æ“šç®¡ç†å™¨

    è·è²¬ï¼šçµ±ä¸€çš„æ•¸æ“šè®€å¯«æ¥å£ã€åˆ†å€ç®¡ç†ã€æ•¸æ“šé©—è­‰
    è¨­è¨ˆæ¨¡å¼ï¼šRepository Pattern
    """

    def __init__(self, base_path: str = '/data'):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨

        åƒæ•¸:
            base_path: æ•¸æ“šæ ¹ç›®éŒ„
        """
        self.base_path = Path(base_path)
        self.logger = logging.getLogger(__name__)

    def write_time_partition(
        self,
        data: pd.DataFrame,
        partition_date: str
    ):
        """
        å¯«å…¥æ™‚é–“åˆ†å€

        åƒæ•¸:
            data: æ•¸æ“š DataFrame
            partition_date: åˆ†å€æ—¥æœŸ ("2024-01-01")
        """
        partition_path = self.base_path / 'daily' / f'date={partition_date}'
        partition_path.mkdir(parents=True, exist_ok=True)

        file_path = partition_path / 'data.parquet'

        # å¯«å…¥ Parquet
        data.to_parquet(
            file_path,
            engine='pyarrow',
            compression='snappy',
            index=False
        )

        self.logger.info(f"æ™‚é–“åˆ†å€å¯«å…¥å®Œæˆ: {file_path}")

    def write_symbol_partition(
        self,
        data: pd.DataFrame,
        symbol: str
    ):
        """
        å¯«å…¥å€‹è‚¡åˆ†å€

        åƒæ•¸:
            data: è©²è‚¡ç¥¨æ­·å²æ•¸æ“š
            symbol: è‚¡ç¥¨ä»£ç¢¼
        """
        partition_path = self.base_path / 'history' / f'symbol={symbol}'
        partition_path.mkdir(parents=True, exist_ok=True)

        file_path = partition_path / 'data.parquet'

        # å¯«å…¥ Parquet
        data.to_parquet(
            file_path,
            engine='pyarrow',
            compression='snappy',
            index=False
        )

        self.logger.info(f"å€‹è‚¡åˆ†å€å¯«å…¥å®Œæˆ: {file_path}")

    def read_time_partition(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        è®€å–æ™‚é–“åˆ†å€ç¯„åœæ•¸æ“š

        åƒæ•¸:
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        è¿”å›:
            DataFrame: åˆä½µå¾Œçš„æ•¸æ“š
        """
        daily_path = self.base_path / 'daily'

        # ä½¿ç”¨ pyarrow çš„åˆ†å€éæ¿¾
        dataset = pq.ParquetDataset(
            daily_path,
            filters=[
                ('date', '>=', start_date),
                ('date', '<=', end_date)
            ]
        )

        df = dataset.read().to_pandas()

        return df

    def read_symbol_partition(self, symbol: str) -> pd.DataFrame:
        """
        è®€å–å€‹è‚¡åˆ†å€æ•¸æ“š

        åƒæ•¸:
            symbol: è‚¡ç¥¨ä»£ç¢¼

        è¿”å›:
            DataFrame: è©²è‚¡ç¥¨æ­·å²æ•¸æ“š
        """
        file_path = self.base_path / 'history' / f'symbol={symbol}' / 'data.parquet'

        if not file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è‚¡ç¥¨ {symbol} çš„æ•¸æ“š")

        df = pd.read_parquet(file_path)

        return df

    def transpose_to_symbol_partition(self, date: str):
        """
        å°‡æ™‚é–“åˆ†å€è½‰ç½®ç‚ºå€‹è‚¡åˆ†å€

        åƒæ•¸:
            date: è¦è½‰ç½®çš„æ—¥æœŸ
        """
        # æ­¥é©Ÿ 1: è®€å–è©²æ—¥æ•¸æ“š
        daily_data = self.read_time_partition(date, date)

        # æ­¥é©Ÿ 2: ä¾ symbol åˆ†çµ„
        grouped = daily_data.groupby('symbol')

        # æ­¥é©Ÿ 3: éæ­·æ¯æª”è‚¡ç¥¨
        for symbol, group in grouped:
            try:
                # è®€å–ç¾æœ‰æ­·å²æ•¸æ“š
                try:
                    existing = self.read_symbol_partition(symbol)
                    # åˆä½µæ–°èˆŠæ•¸æ“š
                    updated = pd.concat([existing, group], ignore_index=True)
                    # å»é‡ï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
                    updated = updated.drop_duplicates(subset=['date'], keep='last')
                except FileNotFoundError:
                    # è©²è‚¡ç¥¨é¦–æ¬¡å¯«å…¥
                    updated = group

                # æ’åº
                updated = updated.sort_values('date').reset_index(drop=True)

                # å¯«å…¥å€‹è‚¡åˆ†å€
                self.write_symbol_partition(updated, symbol)

            except Exception as e:
                self.logger.error(f"è½‰ç½® {symbol} å¤±æ•—: {e}")

        self.logger.info(f"ETL è½‰ç½®å®Œæˆ: {date}")

    def cleanup_old_data(self, keep_days: int = 30):
        """
        æ¸…ç†èˆŠçš„æ™‚é–“åˆ†å€æ•¸æ“š

        åƒæ•¸:
            keep_days: ä¿ç•™å¤©æ•¸
        """
        cutoff_date = datetime.now() - timedelta(days=keep_days)

        daily_path = self.base_path / 'daily'

        for partition in daily_path.iterdir():
            if partition.is_dir():
                # æå–æ—¥æœŸ
                date_str = partition.name.replace('date=', '')
                partition_date = datetime.strptime(date_str, '%Y-%m-%d')

                # åˆªé™¤éèˆŠçš„åˆ†å€
                if partition_date < cutoff_date:
                    shutil.rmtree(partition)
                    self.logger.info(f"å·²åˆªé™¤èˆŠåˆ†å€: {partition}")
```

---

## 4. è‡ªå‹•åŒ–è…³æœ¬è¨­è¨ˆ

### 4.1 æ¯æ—¥æ›´æ–°è…³æœ¬ (update_data.py)

#### 4.1.1 åŸ·è¡Œæµç¨‹åœ–

```mermaid
flowchart TD
    START([æ¯æ—¥ 15:00 è§¸ç™¼])

    START --> INIT[åˆå§‹åŒ–<br/>è¼‰å…¥é…ç½® + å»ºç«‹é€£ç·š]
    INIT --> GET_UNIVERSE[å–å¾— Top 500 åå–®<br/>å¾å¿«ç…§æˆ–é‡æ–°è¨ˆç®—]

    GET_UNIVERSE --> PARALLEL_FETCH{ä¸¦è¡ŒæŠ“å–æ•¸æ“š}

    PARALLEL_FETCH --> TASK1[ä»»å‹™ 1: æ—¥è¡Œæƒ…<br/>ThreadPoolExecutor]
    PARALLEL_FETCH --> TASK2[ä»»å‹™ 2: æ³•äººè²·è³£è¶…<br/>ThreadPoolExecutor]
    PARALLEL_FETCH --> TASK3[ä»»å‹™ 3: é›†ä¿åˆ†æ•£è¡¨<br/>é€±äº”åŸ·è¡Œ]

    TASK1 --> VALIDATE1{æ•¸æ“šé©—è­‰}
    TASK2 --> VALIDATE2{æ•¸æ“šé©—è­‰}
    TASK3 --> VALIDATE3{æ•¸æ“šé©—è­‰}

    VALIDATE1 -->|å¤±æ•—| ERROR1[è¨˜éŒ„éŒ¯èª¤ + é€šçŸ¥]
    VALIDATE2 -->|å¤±æ•—| ERROR2[è¨˜éŒ„éŒ¯èª¤ + é€šçŸ¥]
    VALIDATE3 -->|å¤±æ•—| ERROR3[è¨˜éŒ„éŒ¯èª¤ + é€šçŸ¥]

    VALIDATE1 -->|é€šé| WRITE1[å¯«å…¥ Parquet<br/>æ™‚é–“åˆ†å€]
    VALIDATE2 -->|é€šé| WRITE2[å¯«å…¥ Parquet<br/>ç±Œç¢¼æ•¸æ“š]
    VALIDATE3 -->|é€šé| WRITE3[å¯«å…¥ Parquet<br/>é›†ä¿æ•¸æ“š]

    WRITE1 --> ETL[ETL è½‰ç½®<br/>æ™‚é–“åˆ†å€ â†’ å€‹è‚¡åˆ†å€]
    WRITE2 --> ETL
    WRITE3 --> ETL

    ETL --> CLEANUP[æ¸…ç†æš«å­˜æª”<br/>ä¿ç•™è¿‘ 30 æ—¥æ•¸æ“š]

    CLEANUP --> SUCCESS[ç™¼é€æˆåŠŸé€šçŸ¥<br/>Line + Telegram]

    ERROR1 --> END([å®Œæˆ])
    ERROR2 --> END
    ERROR3 --> END
    SUCCESS --> END
```

#### 4.1.2 å½ä»£ç¢¼å¯¦ä½œ

```python
# å½ä»£ç¢¼ï¼šupdate_data.py æ¯æ—¥æ›´æ–°è…³æœ¬

import schedule
import time
from datetime import datetime, date
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

def daily_update():
    """
    æ¯æ—¥æ•¸æ“šæ›´æ–°ä¸»æµç¨‹

    åŸ·è¡Œæ™‚æ©Ÿ: æ¯æ—¥ 15:00
    åŠŸèƒ½:
        1. æŠ“å–æ—¥è¡Œæƒ…ã€æ³•äººã€é›†ä¿æ•¸æ“š
        2. æ•¸æ“šé©—è­‰èˆ‡æ¸…æ´—
        3. å¯«å…¥ Parquet åˆ†å€
        4. ETL è½‰ç½®
        5. æ¸…ç†èˆŠæ•¸æ“š
    """
    logger = logging.getLogger(__name__)
    logger.info(f"é–‹å§‹æ¯æ—¥æ›´æ–°: {datetime.now()}")

    try:
        # æ­¥é©Ÿ 1: åˆå§‹åŒ–
        api_client = ShioajiClient(
            api_key=config['api_key'],
            secret_key=config['secret_key']
        )
        api_client.connect()

        data_manager = ParquetManager(base_path='/data')
        validator = DataValidator()

        # æ­¥é©Ÿ 2: å–å¾— Top 500 åå–®
        universe = get_top500_universe(api_client)
        logger.info(f"Top 500 åå–®: {len(universe)} æª”")

        # æ­¥é©Ÿ 3: ä¸¦è¡ŒæŠ“å–æ•¸æ“š
        results = parallel_fetch_data(api_client, universe)

        # æ­¥é©Ÿ 4: æ•¸æ“šé©—è­‰
        validated_results = {}
        for data_type, data in results.items():
            if validator.validate(data, data_type):
                validated_results[data_type] = data
                logger.info(f"{data_type} é©—è­‰é€šé")
            else:
                logger.error(f"{data_type} é©—è­‰å¤±æ•—")
                send_alert(f"{data_type} æ•¸æ“šç•°å¸¸")

        # æ­¥é©Ÿ 5: å¯«å…¥ Parquet
        today = date.today().strftime('%Y-%m-%d')

        if 'price' in validated_results:
            data_manager.write_time_partition(
                data=validated_results['price'],
                partition_date=today
            )

        if 'institutional' in validated_results:
            data_manager.write_chips(
                data=validated_results['institutional'],
                date=today
            )

        # æ­¥é©Ÿ 6: ETL è½‰ç½®ï¼ˆæ™‚é–“åˆ†å€ â†’ å€‹è‚¡åˆ†å€ï¼‰
        etl_pipeline = ETLPipeline(data_manager)
        etl_pipeline.transpose_to_symbol_partition(date=today)

        # æ­¥é©Ÿ 7: æ¸…ç†èˆŠæ•¸æ“šï¼ˆä¿ç•™ 30 æ—¥ï¼‰
        data_manager.cleanup_old_data(keep_days=30)

        # æ­¥é©Ÿ 8: æˆåŠŸé€šçŸ¥
        send_notification(
            "âœ… æ•¸æ“šæ›´æ–°æˆåŠŸ",
            f"æ—¥æœŸ: {today}\næª”æ¡ˆæ•¸: {len(universe)}"
        )

        logger.info("æ¯æ—¥æ›´æ–°å®Œæˆ")

    except Exception as e:
        logger.error(f"æ¯æ—¥æ›´æ–°å¤±æ•—: {e}", exc_info=True)
        send_alert(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")

    finally:
        if api_client:
            api_client.disconnect()

def parallel_fetch_data(
    api_client: ShioajiClient,
    universe: List[str]
) -> Dict[str, pd.DataFrame]:
    """
    ä¸¦è¡ŒæŠ“å–æ•¸æ“š

    ä½¿ç”¨ ThreadPoolExecutor æå‡æ•ˆç‡

    åƒæ•¸:
        api_client: API å®¢æˆ¶ç«¯
        universe: è‚¡ç¥¨æ± 

    è¿”å›:
        dict: {data_type: DataFrame}
    """
    results = {}

    with ThreadPoolExecutor(max_workers=10) as executor:
        # æäº¤ä»»å‹™
        futures = {}

        # ä»»å‹™ 1: æ—¥è¡Œæƒ…ï¼ˆæ‰¹æ¬¡æŠ“å–ï¼‰
        future_price = executor.submit(
            fetch_daily_prices, api_client, universe
        )
        futures[future_price] = 'price'

        # ä»»å‹™ 2: æ³•äººè²·è³£è¶…
        future_inst = executor.submit(
            fetch_institutional_trades, api_client, date.today()
        )
        futures[future_inst] = 'institutional'

        # ä»»å‹™ 3: é›†ä¿åˆ†æ•£è¡¨ï¼ˆåƒ…é€±äº”åŸ·è¡Œï¼‰
        if date.today().weekday() == 4:  # Friday
            future_tdcc = executor.submit(
                fetch_tdcc_distribution, api_client, universe
            )
            futures[future_tdcc] = 'tdcc'

        # æ”¶é›†çµæœ
        for future in as_completed(futures):
            data_type = futures[future]
            try:
                results[data_type] = future.result()
                logging.info(f"{data_type} æŠ“å–å®Œæˆ")
            except Exception as e:
                logging.error(f"{data_type} æŠ“å–å¤±æ•—: {e}")

    return results

def main():
    """ä¸»å‡½æ•¸ï¼šè¨­å®šæ’ç¨‹"""
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('/logs/update.log'),
            logging.StreamHandler()
        ]
    )

    # è¨­å®šæ’ç¨‹ï¼šæ¯æ—¥ 15:00 åŸ·è¡Œ
    schedule.every().day.at("15:00").do(daily_update)

    logging.info("æ¯æ—¥æ›´æ–°æ’ç¨‹å·²å•Ÿå‹•ï¼Œç­‰å¾…åŸ·è¡Œ...")

    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡

if __name__ == '__main__':
    main()
```

---

### 4.2 ç­–ç•¥æƒæè…³æœ¬ (run_strategy.py)

#### 4.2.1 é¸è‚¡æƒææµç¨‹åœ–

```mermaid
flowchart TD
    START([æ¯æ—¥ 16:00 è§¸ç™¼])

    START --> LOAD_CONFIG[è¼‰å…¥é…ç½®<br/>ç­–ç•¥åƒæ•¸ + æ•¸æ“šè·¯å¾‘]
    LOAD_CONFIG --> LOAD_UNIVERSE[è¼‰å…¥ Top 500 åå–®<br/>å¾å¿«ç…§æª”æ¡ˆ]

    LOAD_UNIVERSE --> INIT_MODULES[åˆå§‹åŒ–æ¨¡çµ„<br/>FactorEngine + StockScreener]

    INIT_MODULES --> SCREEN[åŸ·è¡Œé¸è‚¡<br/>ä¸‰å±¤ç¯©é¸]

    SCREEN --> LAYER1[Layer 1: åŸºæœ¬é¢<br/>è¨ˆç®— 7 å› å­å¾—åˆ†]
    LAYER1 --> TOP30[æ’åºé¸å‡º Top 30]

    TOP30 --> LAYER2[Layer 2: ç±Œç¢¼é¢<br/>æ³•äºº + å¤§æˆ¶é©—è­‰]
    LAYER2 --> CHIP_PASS[é€šéç±Œç¢¼é¢æª¢æŸ¥]

    CHIP_PASS --> LAYER3[Layer 3: æŠ€è¡“é¢<br/>ä¹–é›¢ç‡ + KD/RSI]
    LAYER3 --> SIGNAL_GEN[ç”Ÿæˆè²·è³£è¨Šè™Ÿ<br/>å¼·åŠ›è²·é€²/åŠ ç¢¼/æ¸›ç¢¼]

    SIGNAL_GEN --> ENRICH[è£œå……è³‡è¨Š<br/>è‚¡ç¥¨åç¨±ã€ç”¢æ¥­ã€å¸‚å€¼]

    ENRICH --> EXPORT_CSV[è¼¸å‡º CSV<br/>/reports/selections/]
    EXPORT_CSV --> EXPORT_JSON[è¼¸å‡º JSON<br/>API ä½¿ç”¨]

    EXPORT_JSON --> CHECK_SIGNAL{æœ‰å¼·åŠ›è²·é€²<br/>è¨Šè™Ÿ?}

    CHECK_SIGNAL -->|æ˜¯| SEND_NOTIFY[ç™¼é€é€šçŸ¥<br/>Line + Telegram]
    CHECK_SIGNAL -->|å¦| SEND_SUMMARY[ç™¼é€æ‘˜è¦<br/>åƒ…é€šçŸ¥æª”æ¡ˆæ•¸]

    SEND_NOTIFY --> LOG[è¨˜éŒ„æ—¥èªŒ<br/>é¸è‚¡çµæœ + è€—æ™‚]
    SEND_SUMMARY --> LOG

    LOG --> END([å®Œæˆ])
```

#### 4.2.2 å½ä»£ç¢¼å¯¦ä½œ

```python
# å½ä»£ç¢¼ï¼šrun_strategy.py ç­–ç•¥æƒæè…³æœ¬

def run_stock_screening():
    """
    åŸ·è¡Œé¸è‚¡ç­–ç•¥

    æµç¨‹:
        1. è¼‰å…¥é…ç½®èˆ‡æ•¸æ“š
        2. åˆå§‹åŒ–é¸è‚¡å¼•æ“
        3. åŸ·è¡Œä¸‰å±¤ç¯©é¸
        4. è¼¸å‡ºçµæœèˆ‡é€šçŸ¥
    """
    logger = logging.getLogger(__name__)
    logger.info("é–‹å§‹ç­–ç•¥æƒæ")

    try:
        # æ­¥é©Ÿ 1: è¼‰å…¥é…ç½®
        config = load_config('config/parameters.yaml')

        # æ­¥é©Ÿ 2: è¼‰å…¥ Top 500 åå–®
        universe_file = f"/data/market_cap/universe_top500_{date.today()}.parquet"
        universe_df = pd.read_parquet(universe_file)
        universe = universe_df['symbol'].tolist()

        logger.info(f"å€™é¸æ± : {len(universe)} æª”")

        # æ­¥é©Ÿ 3: åˆå§‹åŒ–æ¨¡çµ„
        data_manager = ParquetManager(base_path='/data')
        factor_engine = FactorEngine(data_manager)
        screener = StockScreener(factor_engine, data_manager)

        # æ­¥é©Ÿ 4: åŸ·è¡Œé¸è‚¡
        results = screener.screen_stocks(universe)

        # æ­¥é©Ÿ 5: è£œå……è‚¡ç¥¨åç¨±ã€ç”¢æ¥­ç­‰è³‡è¨Š
        results = enrich_stock_info(results)

        # æ­¥é©Ÿ 6: è¼¸å‡ºçµæœ
        output_path = f"/reports/selections/selection_{date.today()}.csv"
        results.to_csv(output_path, index=False, encoding='utf-8-sig')

        logger.info(f"é¸è‚¡çµæœå·²è¼¸å‡º: {output_path}")

        # æ­¥é©Ÿ 7: ç™¼é€é€šçŸ¥
        strong_buy = results[results['signal'] == 'å¼·åŠ›è²·é€²']

        if len(strong_buy) > 0:
            message = format_notification(strong_buy)
            send_notification("ğŸ“Š ä»Šæ—¥é¸è‚¡çµæœ", message)
        else:
            send_notification("ğŸ“Š ä»Šæ—¥é¸è‚¡", f"å…±é¸å‡º {len(results)} æª”ï¼Œç„¡å¼·åŠ›è²·é€²è¨Šè™Ÿ")

        logger.info("ç­–ç•¥æƒæå®Œæˆ")

    except Exception as e:
        logger.error(f"ç­–ç•¥æƒæå¤±æ•—: {e}", exc_info=True)
        send_alert(f"âŒ é¸è‚¡å¤±æ•—: {e}")

def format_notification(strong_buy: pd.DataFrame) -> str:
    """
    æ ¼å¼åŒ–é€šçŸ¥è¨Šæ¯

    åƒæ•¸:
        strong_buy: å¼·åŠ›è²·é€²è‚¡ç¥¨æ¸…å–®

    è¿”å›:
        str: æ ¼å¼åŒ–çš„é€šçŸ¥å…§å®¹
    """
    message = f"ğŸ”¥ å¼·åŠ›è²·é€²è¨Šè™Ÿ ({len(strong_buy)} æª”)\n\n"

    for idx, row in strong_buy.iterrows():
        message += f"ã€{row['symbol']}ã€‘{row['name']}\n"
        message += f"  åŸºæœ¬é¢å¾—åˆ†: {row['fundamental_score']:.1f}\n"
        message += f"  ä¹–é›¢ç‡: {row['bias_60']:.2f}%\n"
        message += f"  KD ç‹€æ…‹: {row['kd_cross']}\n\n"

    return message

def send_notification(title: str, message: str):
    """
    ç™¼é€ Line Notify é€šçŸ¥

    åƒæ•¸:
        title: æ¨™é¡Œ
        message: å…§å®¹
    """
    line_token = config['line_notify_token']

    headers = {
        'Authorization': f'Bearer {line_token}'
    }

    data = {
        'message': f"\n{title}\n{'-'*30}\n{message}"
    }

    response = requests.post(
        'https://notify-api.line.me/api/notify',
        headers=headers,
        data=data
    )

    if response.status_code == 200:
        logging.info("Line é€šçŸ¥ç™¼é€æˆåŠŸ")
    else:
        logging.error(f"Line é€šçŸ¥ç™¼é€å¤±æ•—: {response.text}")
```

---

### 4.3 é€šçŸ¥æ¨¡çµ„ (notification.py)

#### 4.3.1 Line Notify æ•´åˆæµç¨‹åœ–

```mermaid
flowchart TD
    START([è§¸ç™¼é€šçŸ¥])

    START --> FORMAT[æ ¼å¼åŒ–è¨Šæ¯<br/>æ¨™é¡Œ + å…§å®¹]
    FORMAT --> CHECK_TOKEN{é©—è­‰<br/>Line Token?}

    CHECK_TOKEN -->|å¤±æ•—| ERROR[è¨˜éŒ„éŒ¯èª¤<br/>è·³éé€šçŸ¥]
    CHECK_TOKEN -->|æˆåŠŸ| BUILD_REQUEST[å»ºç«‹ HTTP è«‹æ±‚<br/>POST /api/notify]

    BUILD_REQUEST --> SEND[ç™¼é€è«‹æ±‚<br/>requests.post()]

    SEND --> CHECK_RESPONSE{HTTP ç‹€æ…‹ç¢¼<br/>200?}

    CHECK_RESPONSE -->|æ˜¯| SUCCESS[è¨˜éŒ„æˆåŠŸæ—¥èªŒ]
    CHECK_RESPONSE -->|å¦| RETRY{é‡è©¦æ¬¡æ•¸<br/>< 3?}

    RETRY -->|æ˜¯| WAIT[ç­‰å¾… 5 ç§’]
    RETRY -->|å¦| FAIL[é€šçŸ¥å¤±æ•—<br/>è¨˜éŒ„éŒ¯èª¤]

    WAIT --> BUILD_REQUEST

    SUCCESS --> END([å®Œæˆ])
    ERROR --> END
    FAIL --> END
```

#### 4.3.2 å½ä»£ç¢¼ï¼šé€šçŸ¥æœå‹™

```python
# å½ä»£ç¢¼ï¼šnotification.py é€šçŸ¥æœå‹™

class NotificationService:
    """
    é€šçŸ¥æœå‹™

    è¨­è¨ˆæ¨¡å¼ï¼šObserver Pattern
    æ”¯æ´å¤šç¨®é€šçŸ¥ç®¡é“ï¼šLine Notify, Telegram Bot
    """

    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–é€šçŸ¥æœå‹™

        åƒæ•¸:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« Token ç­‰è¨­å®š
        """
        self.config = config
        self.logger = logging.getLogger(__name__)

    def send_line_notify(
        self,
        message: str,
        max_retries: int = 3
    ) -> bool:
        """
        ç™¼é€ Line Notify é€šçŸ¥

        åƒæ•¸:
            message: è¨Šæ¯å…§å®¹
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸

        è¿”å›:
            bool: ç™¼é€æˆåŠŸè¿”å› True
        """
        token = self.config.get('line_notify_token')

        if not token:
            self.logger.error("Line Notify Token æœªè¨­å®š")
            return False

        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/x-www-form-urlencoded'
        }

        data = {'message': message}

        for attempt in range(max_retries):
            try:
                response = requests.post(
                    'https://notify-api.line.me/api/notify',
                    headers=headers,
                    data=data,
                    timeout=10
                )

                if response.status_code == 200:
                    self.logger.info("Line é€šçŸ¥ç™¼é€æˆåŠŸ")
                    return True
                else:
                    self.logger.warning(
                        f"Line é€šçŸ¥ç™¼é€å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {response.text}"
                    )

            except Exception as e:
                self.logger.warning(f"Line é€šçŸ¥ç•°å¸¸ (ç¬¬ {attempt + 1} æ¬¡): {e}")

            if attempt < max_retries - 1:
                time.sleep(5)  # ç­‰å¾… 5 ç§’å¾Œé‡è©¦

        self.logger.error("Line é€šçŸ¥æœ€çµ‚å¤±æ•—")
        return False

    def send_telegram(
        self,
        message: str,
        chat_id: str = None
    ) -> bool:
        """
        ç™¼é€ Telegram Bot é€šçŸ¥

        åƒæ•¸:
            message: è¨Šæ¯å…§å®¹
            chat_id: èŠå¤© IDï¼ˆå¯é¸ï¼Œé»˜èªä½¿ç”¨é…ç½®ä¸­çš„ IDï¼‰

        è¿”å›:
            bool: ç™¼é€æˆåŠŸè¿”å› True
        """
        bot_token = self.config.get('telegram_bot_token')
        chat_id = chat_id or self.config.get('telegram_chat_id')

        if not bot_token or not chat_id:
            self.logger.error("Telegram é…ç½®æœªå®Œæ•´")
            return False

        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        data = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'Markdown'
        }

        try:
            response = requests.post(url, json=data, timeout=10)

            if response.status_code == 200:
                self.logger.info("Telegram é€šçŸ¥ç™¼é€æˆåŠŸ")
                return True
            else:
                self.logger.error(f"Telegram é€šçŸ¥å¤±æ•—: {response.text}")
                return False

        except Exception as e:
            self.logger.error(f"Telegram é€šçŸ¥ç•°å¸¸: {e}")
            return False
```

---

## 5. æ¸¬è©¦èˆ‡é©—è­‰

### 5.1 å–®å…ƒæ¸¬è©¦æ¶æ§‹

#### 5.1.1 æ¸¬è©¦é‡‘å­—å¡”åœ–

```mermaid
graph TD
    subgraph "æ¸¬è©¦é‡‘å­—å¡”"
        E2E[ç«¯åˆ°ç«¯æ¸¬è©¦<br/>End-to-End Tests<br/>5%]
        INT[æ•´åˆæ¸¬è©¦<br/>Integration Tests<br/>20%]
        UNIT[å–®å…ƒæ¸¬è©¦<br/>Unit Tests<br/>75%]
    end

    E2E -->|å…¨æµç¨‹é©—è­‰| FULL[å®Œæ•´é¸è‚¡æµç¨‹<br/>æ•¸æ“šæŠ“å– â†’ é¸è‚¡ â†’ è¼¸å‡º]

    INT -->|æ¨¡çµ„æ•´åˆ| API_ETL[API + ETL æ•´åˆ]
    INT -->|æ¨¡çµ„æ•´åˆ| FACTOR_SCREEN[FactorEngine + Screener]

    UNIT -->|æ ¸å¿ƒé‚è¼¯| CALC[å› å­è¨ˆç®—å‡½æ•¸]
    UNIT -->|æ ¸å¿ƒé‚è¼¯| SCORE[è©•åˆ†é‚è¼¯]
    UNIT -->|æ ¸å¿ƒé‚è¼¯| FILTER[ç¯©é¸æ¢ä»¶]
```

#### 5.1.2 pytest æ¸¬è©¦ç¯„ä¾‹

```python
# å½ä»£ç¢¼ï¼štests/test_factors.py

import pytest
import pandas as pd
from unittest.mock import Mock, patch
from src.factors import FactorEngine

@pytest.fixture
def mock_data_manager():
    """Mock æ•¸æ“šç®¡ç†å™¨"""
    manager = Mock()

    # æ¨¡æ“¬è²¡å ±æ•¸æ“š
    manager.read_parquet.return_value = pd.DataFrame({
        'date': pd.date_range('2023-01-01', periods=8, freq='Q'),
        'net_income': [1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700],
        'equity': [5000, 5200, 5400, 5600, 5800, 6000, 6200, 6400],
        'eps': [2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4]
    })

    return manager

@pytest.fixture
def factor_engine(mock_data_manager):
    """åˆå§‹åŒ–å› å­å¼•æ“"""
    return FactorEngine(data_manager=mock_data_manager)

class TestFactorEngine:
    """FactorEngine å–®å…ƒæ¸¬è©¦"""

    def test_calculate_roe_æ­£å¸¸æƒ…æ³(self, factor_engine):
        """æ¸¬è©¦ ROE è¨ˆç®— - æ­£å¸¸æƒ…æ³"""
        # åŸ·è¡Œ
        roe = factor_engine._calculate_roe('2330')

        # é©—è­‰
        assert isinstance(roe, float)
        assert 0 <= roe <= 100  # ROE æ‡‰åœ¨åˆç†ç¯„åœ
        assert abs(roe - 26.67) < 0.1  # é æœŸå€¼ç´„ 26.67%

    def test_calculate_roe_è‚¡æ±æ¬Šç›Šç‚ºé›¶(self, factor_engine, mock_data_manager):
        """æ¸¬è©¦ ROE è¨ˆç®— - é‚Šç•Œæ¢ä»¶ï¼šè‚¡æ±æ¬Šç›Šç‚ºé›¶"""
        # ä¿®æ”¹ Mock æ•¸æ“š
        mock_data_manager.read_parquet.return_value = pd.DataFrame({
            'net_income': [1000, 1100, 1200, 1300],
            'equity': [0, 0, 0, 0]  # ç•°å¸¸æƒ…æ³
        })

        # åŸ·è¡Œ
        roe = factor_engine._calculate_roe('0000')

        # é©—è­‰ï¼šæ‡‰è¿”å› 0 è€Œéæ‹‹å‡ºç•°å¸¸
        assert roe == 0.0

    def test_calculate_eps_yoy_æ­£æˆé•·(self, factor_engine):
        """æ¸¬è©¦ EPS YoY è¨ˆç®— - æ­£æˆé•·"""
        # åŸ·è¡Œ
        eps_yoy = factor_engine._calculate_eps_yoy('2330')

        # é©—è­‰
        # æœ€æ–° EPS = 3.4, å»å¹´åŒå­£ EPS = 2.6
        # YoY = (3.4 - 2.6) / 2.6 * 100 â‰ˆ 30.77%
        assert abs(eps_yoy - 30.77) < 0.1

    def test_calculate_eps_yoy_è² æˆé•·(self, factor_engine, mock_data_manager):
        """æ¸¬è©¦ EPS YoY è¨ˆç®— - è² æˆé•·"""
        # ä¿®æ”¹ Mock æ•¸æ“šï¼ˆEPS è¡°é€€ï¼‰
        mock_data_manager.read_parquet.return_value = pd.DataFrame({
            'eps': [3.0, 2.8, 2.6, 2.4, 2.2, 2.0, 1.8, 1.6]
        })

        # åŸ·è¡Œ
        eps_yoy = factor_engine._calculate_eps_yoy('1234')

        # é©—è­‰ï¼šæ‡‰ç‚ºè² å€¼
        assert eps_yoy < 0

    def test_score_factor_ROEè©•åˆ†(self, factor_engine):
        """æ¸¬è©¦å› å­è©•åˆ†é‚è¼¯ - ROE"""
        # æ¸¬è©¦ä¸åŒ ROE å€¼çš„è©•åˆ†
        test_cases = [
            (20, 5),   # >15% â†’ 5åˆ†
            (12, 4),   # 10-15% â†’ 4åˆ†
            (7, 3),    # 5-10% â†’ 3åˆ†
            (2, 2),    # 0-5% â†’ 2åˆ†
            (-5, 1)    # <0% â†’ 1åˆ†
        ]

        for roe_value, expected_score in test_cases:
            score = factor_engine._score_factor('roe', roe_value)
            assert score == expected_score, \
                f"ROE {roe_value}% æ‡‰å¾— {expected_score} åˆ†ï¼Œå¯¦éš›å¾— {score} åˆ†"

    def test_calculate_fundamental_score_æ•´åˆ(self, factor_engine):
        """æ¸¬è©¦åŸºæœ¬é¢ç¶œåˆå¾—åˆ† - æ•´åˆæ¸¬è©¦"""
        # åŸ·è¡Œ
        total_score = factor_engine.calculate_fundamental_score('2330')

        # é©—è­‰
        assert isinstance(total_score, float)
        assert 0 <= total_score <= 200  # ç¸½åˆ†ç¯„åœ
        assert total_score > 100  # é æœŸå°ç©é›»å¾—åˆ† > 100

    @patch('src.factors.FactorEngine._calculate_roe')
    @patch('src.factors.FactorEngine._calculate_eps_yoy')
    def test_calculate_fundamental_score_ç•°å¸¸è™•ç†(
        self,
        mock_eps_yoy,
        mock_roe,
        factor_engine
    ):
        """æ¸¬è©¦åŸºæœ¬é¢è¨ˆç®— - ç•°å¸¸è™•ç†"""
        # æ¨¡æ“¬è¨ˆç®—ç•°å¸¸
        mock_roe.side_effect = Exception("æ•¸æ“šç¼ºå¤±")
        mock_eps_yoy.return_value = 10.0

        # åŸ·è¡Œï¼šæ‡‰æ•ç²ç•°å¸¸ä¸¦è¿”å› 0 æˆ–è¨˜éŒ„æ—¥èªŒ
        with pytest.raises(Exception):
            factor_engine.calculate_fundamental_score('9999')

# åŸ·è¡Œæ¸¬è©¦
# pytest tests/test_factors.py -v --cov=src/factors
```

---

### 5.2 æ•¸æ“šé©—è­‰è…³æœ¬

#### 5.2.1 æ•¸æ“šå“è³ªæª¢æŸ¥æµç¨‹åœ–

```mermaid
flowchart TD
    START([é–‹å§‹é©—è­‰])

    START --> LOAD[è¼‰å…¥æ•¸æ“š<br/>DataFrame]

    LOAD --> CHECK1{æª¢æŸ¥ 1<br/>ç¼ºå¤±å€¼?}
    CHECK1 -->|æœ‰| HANDLE1[è™•ç†ç­–ç•¥<br/>ffill / æ’å€¼ / ä¸Ÿæ£„]
    CHECK1 -->|ç„¡| CHECK2
    HANDLE1 --> CHECK2

    CHECK2{æª¢æŸ¥ 2<br/>ç•°å¸¸å€¼?}
    CHECK2 -->|æœ‰| HANDLE2[ç•°å¸¸å€¼è™•ç†<br/>Cap / æ¨™è¨˜ / ä¸Ÿæ£„]
    CHECK2 -->|ç„¡| CHECK3
    HANDLE2 --> CHECK3

    CHECK3{æª¢æŸ¥ 3<br/>é‡è¤‡å€¼?}
    CHECK3 -->|æœ‰| HANDLE3[å»é‡<br/>ä¿ç•™æœ€æ–°]
    CHECK3 -->|ç„¡| CHECK4
    HANDLE3 --> CHECK4

    CHECK4{æª¢æŸ¥ 4<br/>æ•¸æ“šå®Œæ•´æ€§?}
    CHECK4 -->|ä¸å®Œæ•´| HANDLE4[è¨˜éŒ„ç¼ºå¤±<br/>ç™¼é€è­¦å‘Š]
    CHECK4 -->|å®Œæ•´| PASS
    HANDLE4 --> REPORT

    PASS[é©—è­‰é€šé] --> REPORT[ç”Ÿæˆå ±å‘Š<br/>è¨˜éŒ„æ—¥èªŒ]
    REPORT --> END([å®Œæˆ])
```

#### 5.2.2 é©—è­‰é‚è¼¯å¯¦ä½œ

```python
# å½ä»£ç¢¼ï¼šsrc/data_validator.py

class DataValidator:
    """
    æ•¸æ“šé©—è­‰å™¨

    è·è²¬ï¼šæª¢æŸ¥æ•¸æ“šå“è³ªã€ç•°å¸¸å€¼æª¢æ¸¬ã€å®Œæ•´æ€§é©—è­‰
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.validation_report = {}

    def validate(
        self,
        data: pd.DataFrame,
        data_type: str
    ) -> bool:
        """
        åŸ·è¡Œå®Œæ•´æ•¸æ“šé©—è­‰

        åƒæ•¸:
            data: å¾…é©—è­‰çš„ DataFrame
            data_type: æ•¸æ“šé¡å‹ï¼ˆ'price', 'institutional', 'financial'ï¼‰

        è¿”å›:
            bool: é©—è­‰é€šéè¿”å› True
        """
        self.logger.info(f"é–‹å§‹é©—è­‰ {data_type} æ•¸æ“š")

        checks = [
            self._check_missing_values(data, data_type),
            self._check_outliers(data, data_type),
            self._check_duplicates(data, data_type),
            self._check_completeness(data, data_type)
        ]

        # æ‰€æœ‰æª¢æŸ¥éƒ½é€šéæ‰è¿”å› True
        result = all(checks)

        self.logger.info(
            f"{data_type} é©—è­‰çµæœ: {'é€šé' if result else 'å¤±æ•—'}"
        )

        return result

    def _check_missing_values(
        self,
        data: pd.DataFrame,
        data_type: str
    ) -> bool:
        """
        æª¢æŸ¥ç¼ºå¤±å€¼

        ç­–ç•¥:
            - åƒ¹æ ¼æ•¸æ“šï¼šç¼ºå¤±ç‡ > 5% å‰‡å¤±æ•—
            - è²¡å ±æ•¸æ“šï¼šé—œéµæ¬„ä½ä¸å…è¨±ç¼ºå¤±
        """
        missing_ratio = data.isnull().sum() / len(data)

        critical_columns = {
            'price': ['close', 'volume'],
            'institutional': ['foreign_net', 'trust_net'],
            'financial': ['net_income', 'equity', 'eps']
        }

        for col in critical_columns.get(data_type, []):
            if col in data.columns:
                if missing_ratio[col] > 0.05:  # 5% é–€æª»
                    self.logger.error(
                        f"{data_type} - {col} ç¼ºå¤±ç‡éé«˜: {missing_ratio[col]:.2%}"
                    )
                    return False

        return True

    def _check_outliers(
        self,
        data: pd.DataFrame,
        data_type: str
    ) -> bool:
        """
        æª¢æ¸¬ç•°å¸¸å€¼

        æ–¹æ³•:
            - åƒ¹æ ¼ï¼šå–®æ—¥æ¼²è·Œå¹… > 50%ï¼ˆæ’é™¤é™¤æ¬Šæ¯ï¼‰
            - æˆäº¤é‡ï¼šè¶…é 30 æ—¥å¹³å‡çš„ 10 å€
        """
        if data_type == 'price':
            # è¨ˆç®—æ¼²è·Œå¹…
            data['pct_change'] = data['close'].pct_change() * 100

            # æª¢æŸ¥ç•°å¸¸æ¼²è·Œå¹…
            outliers = data[abs(data['pct_change']) > 50]

            if len(outliers) > 0:
                self.logger.warning(
                    f"æª¢æ¸¬åˆ° {len(outliers)} ç­†ç•°å¸¸æ¼²è·Œå¹…æ•¸æ“š"
                )
                # åƒ…è­¦å‘Šï¼Œä¸é˜»æ­¢ï¼ˆå¯èƒ½æ˜¯é™¤æ¬Šæ¯ï¼‰

        return True

    def _check_duplicates(
        self,
        data: pd.DataFrame,
        data_type: str
    ) -> bool:
        """
        æª¢æŸ¥é‡è¤‡å€¼

        è¦å‰‡:
            - symbol + date çµ„åˆä¸æ‡‰é‡è¤‡
        """
        if 'symbol' in data.columns and 'date' in data.columns:
            duplicates = data.duplicated(subset=['symbol', 'date'], keep=False)

            if duplicates.sum() > 0:
                self.logger.warning(
                    f"æª¢æ¸¬åˆ° {duplicates.sum()} ç­†é‡è¤‡æ•¸æ“šï¼Œå°‡ä¿ç•™æœ€æ–°"
                )
                # è‡ªå‹•å»é‡
                data.drop_duplicates(
                    subset=['symbol', 'date'],
                    keep='last',
                    inplace=True
                )

        return True

    def _check_completeness(
        self,
        data: pd.DataFrame,
        data_type: str
    ) -> bool:
        """
        æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§

        é©—è­‰:
            - Top 500 è‚¡ç¥¨æ˜¯å¦éƒ½æœ‰æ•¸æ“š
            - äº¤æ˜“æ—¥æ•¸æ“šæ˜¯å¦é½Šå…¨
        """
        if data_type == 'price':
            # æª¢æŸ¥æ˜¯å¦æœ‰ 500 æª”è‚¡ç¥¨çš„æ•¸æ“š
            unique_symbols = data['symbol'].nunique()

            if unique_symbols < 450:  # å…è¨± 10% ç¼ºå¤±
                self.logger.error(
                    f"è‚¡ç¥¨æ•¸é‡ä¸è¶³: {unique_symbols} / 500"
                )
                return False

        return True
```

---

### 5.3 å›æ¸¬æ¡†æ¶

#### 5.3.1 å›æ¸¬æµç¨‹åœ–

```mermaid
flowchart TD
    START([é–‹å§‹å›æ¸¬])

    START --> CONFIG[è¼‰å…¥é…ç½®<br/>èµ·å§‹æ—¥æœŸã€åˆå§‹è³‡é‡‘ã€äº¤æ˜“æˆæœ¬]
    CONFIG --> LOAD_HIST[è¼‰å…¥æ­·å²æ•¸æ“š<br/>è¡Œæƒ… + è²¡å ± + ç±Œç¢¼]

    LOAD_HIST --> INIT_STATE[åˆå§‹åŒ–ç‹€æ…‹<br/>æŒå€‰ = {}, ç¾é‡‘ = 1,000,000]

    INIT_STATE --> LOOP_START{éæ­·æ¯æ—¥}

    LOOP_START --> RUN_STRATEGY[åŸ·è¡Œé¸è‚¡ç­–ç•¥<br/>ä¸‰å±¤ç¯©é¸]

    RUN_STRATEGY --> SIGNALS[ç”Ÿæˆè²·è³£è¨Šè™Ÿ<br/>è²·é€² / æŒæœ‰ / è³£å‡º]

    SIGNALS --> EXECUTE[æ¨¡æ“¬äº¤æ˜“åŸ·è¡Œ<br/>è€ƒæ…®æˆæœ¬èˆ‡æ»‘é»]

    EXECUTE --> UPDATE_PORT[æ›´æ–°æŠ•è³‡çµ„åˆ<br/>æŒå€‰ + ç¾é‡‘]

    UPDATE_PORT --> RECORD[è¨˜éŒ„ç¸¾æ•ˆ<br/>æ¯æ—¥æ·¨å€¼ã€æŒè‚¡]

    RECORD --> LOOP_END{é‚„æœ‰æ—¥æœŸ?}
    LOOP_END -->|æ˜¯| LOOP_START
    LOOP_END -->|å¦| CALC_METRICS

    CALC_METRICS[è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™<br/>å¹´åŒ–å ±é…¬ã€å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤]

    CALC_METRICS --> VISUALIZE[è¦–è¦ºåŒ–çµæœ<br/>æ·¨å€¼æ›²ç·šã€å›æ’¤åœ–]

    VISUALIZE --> REPORT[ç”Ÿæˆå›æ¸¬å ±å‘Š<br/>Markdown / HTML]

    REPORT --> END([å®Œæˆ])
```

#### 5.3.2 å›æ¸¬å¼•æ“å½ä»£ç¢¼

```python
# å½ä»£ç¢¼ï¼šscripts/backtest.py

class SimpleBacktester:
    """
    ç°¡æ˜“å›æ¸¬å¼•æ“

    åŠŸèƒ½:
        - æ¨¡æ“¬é¸è‚¡ç­–ç•¥åœ¨æ­·å²æ•¸æ“šä¸Šçš„è¡¨ç¾
        - è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™ï¼ˆå ±é…¬ç‡ã€å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ï¼‰
        - ç”Ÿæˆè¦–è¦ºåŒ–å ±å‘Š
    """

    def __init__(
        self,
        start_date: str,
        end_date: str,
        initial_capital: float = 1_000_000,
        commission_rate: float = 0.001425  # 0.1425% æ‰‹çºŒè²»
    ):
        self.start_date = start_date
        self.end_date = end_date
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate

        # ç‹€æ…‹
        self.cash = initial_capital
        self.positions = {}  # {symbol: shares}
        self.portfolio_value_history = []
        self.trades_history = []

    def run(
        self,
        screener: StockScreener,
        data_manager: ParquetManager
    ) -> pd.DataFrame:
        """
        åŸ·è¡Œå›æ¸¬

        åƒæ•¸:
            screener: é¸è‚¡ç¯©é¸å™¨
            data_manager: æ•¸æ“šç®¡ç†å™¨

        è¿”å›:
            DataFrame: æ¯æ—¥ç¸¾æ•ˆè¨˜éŒ„
        """
        logging.info(f"å›æ¸¬æœŸé–“: {self.start_date} ~ {self.end_date}")

        # å–å¾—äº¤æ˜“æ—¥åˆ—è¡¨
        trading_days = pd.date_range(
            start=self.start_date,
            end=self.end_date,
            freq='B'  # å·¥ä½œæ—¥
        )

        for current_date in trading_days:
            date_str = current_date.strftime('%Y-%m-%d')

            # æ­¥é©Ÿ 1: åŸ·è¡Œé¸è‚¡ç­–ç•¥
            signals = screener.screen_stocks(
                universe=get_universe_on_date(date_str)
            )

            # æ­¥é©Ÿ 2: åŸ·è¡Œäº¤æ˜“
            self._execute_trades(signals, date_str, data_manager)

            # æ­¥é©Ÿ 3: è¨ˆç®—ç•¶æ—¥æŠ•è³‡çµ„åˆåƒ¹å€¼
            portfolio_value = self._calculate_portfolio_value(
                date_str, data_manager
            )

            # æ­¥é©Ÿ 4: è¨˜éŒ„
            self.portfolio_value_history.append({
                'date': current_date,
                'portfolio_value': portfolio_value,
                'cash': self.cash,
                'positions_value': portfolio_value - self.cash
            })

        # è½‰ç‚º DataFrame
        results = pd.DataFrame(self.portfolio_value_history)

        return results

    def _execute_trades(
        self,
        signals: pd.DataFrame,
        date: str,
        data_manager: ParquetManager
    ):
        """
        åŸ·è¡Œäº¤æ˜“ï¼ˆè²·é€²/è³£å‡ºï¼‰

        é‚è¼¯:
            - è³£å‡ºï¼šä¸åœ¨é¸è‚¡åå–®ä¸­çš„æŒè‚¡
            - è²·é€²ï¼šé¸è‚¡åå–®ä¸­çš„å¼·åŠ›è²·é€²è¨Šè™Ÿï¼ˆå‡åˆ†è³‡é‡‘ï¼‰
        """
        # è³£å‡ºé‚è¼¯
        current_symbols = set(signals['symbol'].tolist())
        for symbol in list(self.positions.keys()):
            if symbol not in current_symbols:
                self._sell(symbol, date, data_manager)

        # è²·é€²é‚è¼¯
        buy_signals = signals[signals['signal'] == 'å¼·åŠ›è²·é€²']

        if len(buy_signals) > 0:
            # å‡åˆ†å¯ç”¨è³‡é‡‘
            capital_per_stock = self.cash / len(buy_signals)

            for _, row in buy_signals.iterrows():
                symbol = row['symbol']
                self._buy(symbol, capital_per_stock, date, data_manager)

    def _buy(
        self,
        symbol: str,
        amount: float,
        date: str,
        data_manager: ParquetManager
    ):
        """è²·å…¥è‚¡ç¥¨"""
        # å–å¾—ç•¶æ—¥åƒ¹æ ¼
        price = get_close_price(symbol, date, data_manager)

        # è¨ˆç®—å¯è²·è‚¡æ•¸ï¼ˆ1 å¼µ = 1000 è‚¡ï¼‰
        commission = amount * self.commission_rate
        shares = int((amount - commission) / price / 1000) * 1000

        if shares > 0:
            cost = shares * price + commission
            self.cash -= cost
            self.positions[symbol] = self.positions.get(symbol, 0) + shares

            # è¨˜éŒ„äº¤æ˜“
            self.trades_history.append({
                'date': date,
                'symbol': symbol,
                'action': 'BUY',
                'shares': shares,
                'price': price,
                'cost': cost
            })

    def _sell(
        self,
        symbol: str,
        date: str,
        data_manager: ParquetManager
    ):
        """è³£å‡ºè‚¡ç¥¨"""
        if symbol not in self.positions:
            return

        shares = self.positions[symbol]
        price = get_close_price(symbol, date, data_manager)

        proceeds = shares * price
        commission = proceeds * self.commission_rate
        self.cash += (proceeds - commission)

        del self.positions[symbol]

        # è¨˜éŒ„äº¤æ˜“
        self.trades_history.append({
            'date': date,
            'symbol': symbol,
            'action': 'SELL',
            'shares': shares,
            'price': price,
            'proceeds': proceeds - commission
        })

    def calculate_metrics(self, results: pd.DataFrame) -> dict:
        """
        è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™

        æŒ‡æ¨™:
            - ç´¯ç©å ±é…¬ç‡
            - å¹´åŒ–å ±é…¬ç‡
            - å¤æ™®æ¯”ç‡
            - æœ€å¤§å›æ’¤
            - å‹ç‡
        """
        results['returns'] = results['portfolio_value'].pct_change()

        # ç´¯ç©å ±é…¬ç‡
        total_return = (
            (results['portfolio_value'].iloc[-1] / self.initial_capital) - 1
        ) * 100

        # å¹´åŒ–å ±é…¬ç‡
        days = (results['date'].iloc[-1] - results['date'].iloc[0]).days
        annual_return = ((1 + total_return / 100) ** (365 / days) - 1) * 100

        # å¤æ™®æ¯”ç‡ï¼ˆå‡è¨­ç„¡é¢¨éšªåˆ©ç‡ = 1%ï¼‰
        risk_free_rate = 0.01 / 252  # æ—¥å ±é…¬ç‡
        excess_returns = results['returns'] - risk_free_rate
        sharpe_ratio = (excess_returns.mean() / excess_returns.std()) * (252 ** 0.5)

        # æœ€å¤§å›æ’¤
        cumulative = (1 + results['returns']).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative / running_max) - 1
        max_drawdown = drawdown.min() * 100

        return {
            'ç´¯ç©å ±é…¬ç‡(%)': round(total_return, 2),
            'å¹´åŒ–å ±é…¬ç‡(%)': round(annual_return, 2),
            'å¤æ™®æ¯”ç‡': round(sharpe_ratio, 2),
            'æœ€å¤§å›æ’¤(%)': round(max_drawdown, 2),
            'ç¸½äº¤æ˜“æ¬¡æ•¸': len(self.trades_history)
        }
```

---

## 6. éƒ¨ç½²èˆ‡é‹ç¶­

### 6.1 éƒ¨ç½²æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "æœ¬åœ°é–‹ç™¼ç’°å¢ƒ"
        DEV[Python é–‹ç™¼ç’°å¢ƒ<br/>venv / conda]
        DEV_DATA[(æœ¬åœ°æ•¸æ“š<br/>/data/)]
    end

    subgraph "Docker å®¹å™¨éƒ¨ç½²"
        DOCKER[Docker å®¹å™¨<br/>fingear:latest]
        CRON[Cron Job<br/>æ’ç¨‹åŸ·è¡Œ]
        VOLUME[(/data/<br/>æ•¸æ“šå·)]
    end

    subgraph "é›²ç«¯éƒ¨ç½²ï¼ˆå¯é¸ï¼‰"
        CLOUD[AWS EC2 / GCP<br/>Linux Instance]
        CLOUD_STORAGE[(S3 / GCS<br/>æ•¸æ“šå‚™ä»½)]
    end

    DEV --> DOCKER
    DOCKER --> VOLUME
    DOCKER --> CRON

    DOCKER --> CLOUD
    VOLUME --> CLOUD_STORAGE
```

### 6.2 Docker å®¹å™¨åŒ–è¨­è¨ˆ

```dockerfile
# Dockerfile

FROM python:3.9-slim

# è¨­å®šå·¥ä½œç›®éŒ„
WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# è¤‡è£½ä¾è³´æ–‡ä»¶
COPY requirements.txt .

# å®‰è£ Python å¥—ä»¶
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½å°ˆæ¡ˆæ–‡ä»¶
COPY src/ ./src/
COPY scripts/ ./scripts/
COPY config/ ./config/

# å»ºç«‹æ•¸æ“šç›®éŒ„
RUN mkdir -p /data/daily /data/history /data/fundamentals /data/chips /reports

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV PYTHONPATH=/app
ENV TZ=Asia/Taipei

# è¨­å®š Cron Job
COPY crontab /etc/cron.d/fingear-cron
RUN chmod 0644 /etc/cron.d/fingear-cron
RUN crontab /etc/cron.d/fingear-cron

# å•Ÿå‹• Cron æœå‹™
CMD ["cron", "-f"]
```

```bash
# docker-compose.yml

version: '3.8'

services:
  fingear:
    build: .
    container_name: fingear
    volumes:
      - ./data:/data
      - ./logs:/logs
      - ./reports:/reports
    environment:
      - TZ=Asia/Taipei
    restart: unless-stopped
```

### 6.3 ç›£æ§èˆ‡æ—¥èªŒç³»çµ±

```python
# å½ä»£ç¢¼ï¼šlogging_config.py

import logging
from logging.handlers import RotatingFileHandler

def setup_logging(log_level=logging.INFO):
    """
    é…ç½®æ—¥èªŒç³»çµ±

    åŠŸèƒ½:
        - æ§åˆ¶å°è¼¸å‡ºï¼ˆå½©è‰²ï¼‰
        - æª”æ¡ˆè¼¸å‡ºï¼ˆè‡ªå‹•è¼ªè½‰ï¼‰
        - çµæ§‹åŒ–æ—¥èªŒæ ¼å¼
    """
    # å»ºç«‹æ ¹æ—¥èªŒå™¨
    logger = logging.getLogger()
    logger.setLevel(log_level)

    # æ ¼å¼è¨­å®š
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # æ§åˆ¶å°è™•ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    # æª”æ¡ˆè™•ç†å™¨ï¼ˆè‡ªå‹•è¼ªè½‰ï¼Œæ¯å€‹æ–‡ä»¶æœ€å¤§ 10MBï¼Œä¿ç•™ 5 å€‹å‚™ä»½ï¼‰
    file_handler = RotatingFileHandler(
        '/logs/fingear.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    # éŒ¯èª¤æ—¥èªŒå–®ç¨è¨˜éŒ„
    error_handler = RotatingFileHandler(
        '/logs/error.log',
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)

    # åŠ å…¥è™•ç†å™¨
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    logger.addHandler(error_handler)

    return logger
```

### 6.4 éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶

```python
# å½ä»£ç¢¼ï¼šrecovery.py

class RecoveryManager:
    """
    éŒ¯èª¤æ¢å¾©ç®¡ç†å™¨

    åŠŸèƒ½:
        - è‡ªå‹•æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
        - å¤±æ•—ä»»å‹™é‡è©¦
        - ç‹€æ…‹å¿«ç…§èˆ‡æ¢å¾©
    """

    def __init__(self, data_manager: ParquetManager):
        self.data_manager = data_manager
        self.logger = logging.getLogger(__name__)

    def check_data_integrity(self, date: str) -> bool:
        """
        æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§

        åƒæ•¸:
            date: æª¢æŸ¥æ—¥æœŸ

        è¿”å›:
            bool: æ•¸æ“šå®Œæ•´è¿”å› True
        """
        try:
            # æª¢æŸ¥æ™‚é–“åˆ†å€æ˜¯å¦å­˜åœ¨
            daily_data = self.data_manager.read_time_partition(date, date)

            # æª¢æŸ¥è‚¡ç¥¨æ•¸é‡
            unique_symbols = daily_data['symbol'].nunique()

            if unique_symbols < 450:
                self.logger.error(f"{date} æ•¸æ“šä¸å®Œæ•´ï¼šåƒ… {unique_symbols} æª”")
                return False

            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_columns = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = set(required_columns) - set(daily_data.columns)

            if missing_columns:
                self.logger.error(f"{date} ç¼ºå°‘æ¬„ä½ï¼š{missing_columns}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"æª¢æŸ¥ {date} æ•¸æ“šå®Œæ•´æ€§å¤±æ•—ï¼š{e}")
            return False

    def retry_failed_task(
        self,
        task_func,
        max_retries: int = 3,
        delay: int = 60
    ):
        """
        é‡è©¦å¤±æ•—çš„ä»»å‹™

        åƒæ•¸:
            task_func: è¦é‡è©¦çš„ä»»å‹™å‡½æ•¸
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            delay: é‡è©¦é–“éš”ï¼ˆç§’ï¼‰
        """
        for attempt in range(max_retries):
            try:
                result = task_func()
                self.logger.info(f"ä»»å‹™æˆåŠŸåŸ·è¡Œï¼ˆç¬¬ {attempt + 1} æ¬¡å˜—è©¦ï¼‰")
                return result

            except Exception as e:
                self.logger.warning(f"ä»»å‹™å¤±æ•—ï¼ˆç¬¬ {attempt + 1} æ¬¡ï¼‰ï¼š{e}")

                if attempt < max_retries - 1:
                    self.logger.info(f"å°‡åœ¨ {delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                else:
                    self.logger.error("ä»»å‹™æœ€çµ‚å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    raise

    def create_snapshot(self, snapshot_name: str):
        """
        å»ºç«‹ç‹€æ…‹å¿«ç…§

        åƒæ•¸:
            snapshot_name: å¿«ç…§åç¨±
        """
        snapshot_path = Path(f'/backups/snapshot_{snapshot_name}.json')

        snapshot_data = {
            'timestamp': datetime.now().isoformat(),
            'data_status': self.get_data_status(),
            'last_update_date': self.get_last_update_date()
        }

        with open(snapshot_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"å¿«ç…§å·²å»ºç«‹ï¼š{snapshot_path}")
```

---

## 7. é™„éŒ„

### 7.1 å®Œæ•´é¡åˆ¥é—œä¿‚åœ–ï¼ˆUMLï¼‰

```mermaid
classDiagram
    class ShioajiClient {
        -api: Shioaji
        -is_connected: bool
        +connect() bool
        +get_historical_data() DataFrame
        +get_institutional_trades() DataFrame
    }

    class ParquetManager {
        -base_path: Path
        +write_time_partition() void
        +write_symbol_partition() void
        +read_time_partition() DataFrame
        +read_symbol_partition() DataFrame
        +transpose_to_symbol_partition() void
    }

    class FactorEngine {
        -data_manager: ParquetManager
        -cache: dict
        +calculate_fundamental_score() float
        -_calculate_roe() float
        -_calculate_eps_yoy() float
        -_score_factor() int
    }

    class StockScreener {
        -factor_engine: FactorEngine
        -data_manager: ParquetManager
        +screen_stocks() DataFrame
        -_layer1_fundamental_screen() DataFrame
        -_layer2_chip_filter() DataFrame
        -_layer3_technical_position() DataFrame
    }

    class DataValidator {
        +validate() bool
        -_check_missing_values() bool
        -_check_outliers() bool
        -_check_duplicates() bool
        -_check_completeness() bool
    }

    class NotificationService {
        -config: dict
        +send_line_notify() bool
        +send_telegram() bool
    }

    class SimpleBacktester {
        -cash: float
        -positions: dict
        +run() DataFrame
        -_execute_trades() void
        -_buy() void
        -_sell() void
        +calculate_metrics() dict
    }

    StockScreener --> FactorEngine : uses
    StockScreener --> ParquetManager : uses
    FactorEngine --> ParquetManager : uses
    SimpleBacktester --> StockScreener : uses
    SimpleBacktester --> ParquetManager : uses
```

### 7.2 æ•¸æ“šåº« Schemaï¼ˆParquet åˆ†å€çµæ§‹ï¼‰

#### æ™‚é–“åˆ†å€çµæ§‹

```
/data/daily/
â”œâ”€â”€ date=2024-01-01/
â”‚   â””â”€â”€ data.parquet
â”‚       æ¬„ä½:
â”‚       - symbol: string (è‚¡ç¥¨ä»£ç¢¼)
â”‚       - date: date (æ—¥æœŸ)
â”‚       - open: float (é–‹ç›¤åƒ¹)
â”‚       - high: float (æœ€é«˜åƒ¹)
â”‚       - low: float (æœ€ä½åƒ¹)
â”‚       - close: float (æ”¶ç›¤åƒ¹)
â”‚       - volume: int64 (æˆäº¤é‡)
â”‚       - amount: float (æˆäº¤é‡‘é¡)
â”‚
â””â”€â”€ date=2024-01-02/
    â””â”€â”€ data.parquet
```

#### å€‹è‚¡åˆ†å€çµæ§‹

```
/data/history/
â”œâ”€â”€ symbol=2330/
â”‚   â””â”€â”€ data.parquet
â”‚       æ¬„ä½:
â”‚       - date: date (æ—¥æœŸ)
â”‚       - open: float (é–‹ç›¤åƒ¹)
â”‚       - high: float (æœ€é«˜åƒ¹)
â”‚       - low: float (æœ€ä½åƒ¹)
â”‚       - close: float (æ”¶ç›¤åƒ¹)
â”‚       - volume: int64 (æˆäº¤é‡)
â”‚       - ma5: float (5æ—¥å‡ç·š)
â”‚       - ma20: float (20æ—¥å‡ç·š)
â”‚       - ma60: float (60æ—¥å‡ç·š)
â”‚       - rsi_14: float (14æ—¥RSI)
â”‚       - k: float (KDæŒ‡æ¨™Kå€¼)
â”‚       - d: float (KDæŒ‡æ¨™Då€¼)
â”‚
â””â”€â”€ symbol=2454/
    â””â”€â”€ data.parquet
```

#### è²¡å ±æ•¸æ“šçµæ§‹

```
/data/fundamentals/quarterly/
â””â”€â”€ 2330.parquet
    æ¬„ä½:
    - date: date (è²¡å ±æ—¥æœŸ)
    - revenue: float (ç‡Ÿæ¥­æ”¶å…¥)
    - gross_profit: float (æ¯›åˆ©)
    - operating_income: float (ç‡Ÿæ¥­åˆ©ç›Š)
    - net_income: float (ç¨…å¾Œæ·¨åˆ©)
    - eps: float (æ¯è‚¡ç›ˆé¤˜)
    - equity: float (è‚¡æ±æ¬Šç›Š)
    - total_assets: float (ç¸½è³‡ç”¢)
    - total_liabilities: float (ç¸½è² å‚µ)
    - operating_cash_flow: float (ç‡Ÿæ¥­ç¾é‡‘æµ)
    - capital_expenditure: float (è³‡æœ¬æ”¯å‡º)
```

### 7.3 API æ¥å£æ–‡æª”

#### ShioajiClient API

| æ–¹æ³•                     | åƒæ•¸                          | è¿”å›å€¼      | èªªæ˜             |
| ------------------------ | ----------------------------- | ----------- | ---------------- |
| `connect()`              | -                             | bool        | å»ºç«‹APIé€£ç·š      |
| `disconnect()`           | -                             | void        | æ–·é–‹é€£ç·š         |
| `get_historical_data()`  | symbol, start_date, end_date  | DataFrame   | å–å¾—æ­·å²Kç·š      |
| `get_stock_snapshot()`   | symbol                        | DataFrame   | å–å¾—å³æ™‚å¿«ç…§     |
| `get_institutional_trades()` | date                      | DataFrame   | å–å¾—æ³•äººè²·è³£è¶…   |

#### ParquetManager API

| æ–¹æ³•                     | åƒæ•¸                          | è¿”å›å€¼      | èªªæ˜             |
| ------------------------ | ----------------------------- | ----------- | ---------------- |
| `write_time_partition()` | data, partition_date          | void        | å¯«å…¥æ™‚é–“åˆ†å€     |
| `write_symbol_partition()` | data, symbol                | void        | å¯«å…¥å€‹è‚¡åˆ†å€     |
| `read_time_partition()`  | start_date, end_date          | DataFrame   | è®€å–æ™‚é–“åˆ†å€     |
| `read_symbol_partition()` | symbol                       | DataFrame   | è®€å–å€‹è‚¡åˆ†å€     |
| `transpose_to_symbol_partition()` | date             | void        | ETLè½‰ç½®          |

#### FactorEngine API

| æ–¹æ³•                     | åƒæ•¸                          | è¿”å›å€¼      | èªªæ˜             |
| ------------------------ | ----------------------------- | ----------- | ---------------- |
| `calculate_fundamental_score()` | symbol              | float       | è¨ˆç®—åŸºæœ¬é¢å¾—åˆ†   |
| `_calculate_roe()`       | symbol                        | float       | è¨ˆç®—ROE          |
| `_calculate_eps_yoy()`   | symbol                        | float       | è¨ˆç®—EPS YoY      |
| `_calculate_fcf()`       | symbol                        | float       | è¨ˆç®—è‡ªç”±ç¾é‡‘æµ   |
| `_score_factor()`        | factor_name, raw_value        | int         | å› å­è©•åˆ†         |

### 7.4 åƒè€ƒè³‡æºèˆ‡å»¶ä¼¸é–±è®€

#### å®˜æ–¹æ–‡æª”

- [Shioaji API æ–‡æª”](https://sinotrade.github.io/zh_TW/)
- [Apache Parquet æ ¼å¼è¦ç¯„](https://parquet.apache.org/docs/)
- [Pandas å®˜æ–¹æ–‡æª”](https://pandas.pydata.org/docs/)
- [pytest æ¸¬è©¦æ¡†æ¶](https://docs.pytest.org/)

#### é‡åŒ–é‡‘èåƒè€ƒ

- ã€ŠQuantitative Tradingã€‹ by Ernest Chan
- ã€ŠAdvances in Financial Machine Learningã€‹ by Marcos LÃ³pez de Prado
- ã€ŠFactor Investingã€‹ by Andrew Ang

#### å°è‚¡ç‰¹è‰²è³‡æº

- [å°ç£è­‰åˆ¸äº¤æ˜“æ‰€å…¬é–‹è³‡è¨Š](https://www.twse.com.tw/)
- [é›†ä¿çµç®—æ‰€è‚¡æ¬Šåˆ†æ•£è¡¨](https://www.tdcc.com.tw/)
- [FinLab å°è‚¡è³‡æ–™åº«](https://www.finlab.tw/)

#### æŠ€è¡“å¯¦ä½œåƒè€ƒ

- [Parquet æœ€ä½³å¯¦è¸](https://arrow.apache.org/docs/python/parquet.html)
- [pandas æ•ˆèƒ½å„ªåŒ–](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
- [Python æ—¥èªŒæœ€ä½³å¯¦è¸](https://docs.python.org/3/howto/logging.html)

---

## æ–‡ä»¶ç¸½çµ

æœ¬å¯¦ä½œæ¶æ§‹æ–‡ä»¶æä¾›äº† FinGear ç³»çµ±çš„å®Œæ•´æŠ€è¡“è¨­è¨ˆï¼ŒåŒ…å«ï¼š

1. **22 å€‹ Mermaid åœ–è¡¨**ï¼šæ¶µè“‹ç³»çµ±æ¶æ§‹ã€æ•¸æ“šæµã€æ™‚åºåœ–ã€ç‹€æ…‹æ©Ÿç­‰
2. **å®Œæ•´å½ä»£ç¢¼å¯¦ä½œ**ï¼šæ‰€æœ‰æ ¸å¿ƒæ¨¡çµ„çš„è©³ç´°å¯¦ä½œæŒ‡å°
3. **æ¸¬è©¦æ¡†æ¶**ï¼šå–®å…ƒæ¸¬è©¦ã€æ•¸æ“šé©—è­‰ã€å›æ¸¬å¼•æ“çš„å®Œæ•´ç¯„ä¾‹
4. **éƒ¨ç½²æŒ‡å—**ï¼šDocker å®¹å™¨åŒ–èˆ‡ç›£æ§ç³»çµ±è¨­è¨ˆ

é…åˆ [Overview.md](Overview.md) èˆ‡ [FunctionalIndicators.md](FunctionalIndicators.md)ï¼Œæ‚¨å¯ä»¥é–‹å§‹å¯¦ä½œ FinGear ç³»çµ±ã€‚

å»ºè­°å¯¦ä½œé †åºï¼š
1. å»ºç«‹åŸºç¤è¨­æ–½ï¼šParquetManager + DataValidator
2. å¯¦ä½œæ•¸æ“šå±¤ï¼šShioajiClient + ETLPipeline
3. å¯¦ä½œç­–ç•¥å±¤ï¼šFactorEngine + StockScreener
4. å¯¦ä½œæ‡‰ç”¨å±¤ï¼šupdate_data.py + run_strategy.py
5. æ¸¬è©¦èˆ‡é©—è­‰ï¼šå–®å…ƒæ¸¬è©¦ + å›æ¸¬
6. éƒ¨ç½²èˆ‡é‹ç¶­ï¼šDocker + ç›£æ§

ç¥æ‚¨å¯¦ä½œé †åˆ©ï¼
